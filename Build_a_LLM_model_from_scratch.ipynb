{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rdEDmLGzvhOQ",
        "QuA8LjeDEyjo",
        "b-gimTPSWDgU",
        "pok0dHxYuHQc"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOKnuHWAxZxT2tkHDsLt5xI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BonanYang/LLM-study/blob/main/Build_a_LLM_model_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 2**"
      ],
      "metadata": {
        "id": "rdEDmLGzvhOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiBPNxpW_7w2",
        "outputId": "2946a6f8-4fa4-4b20-8682-d7a5175384b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "# 2.2 Tokenizing\n",
        "import re\n",
        "with open('the-verdict.txt') as f:\n",
        "  raw = f.read()\n",
        "  # print(len(raw))\n",
        "\n",
        "res = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw)\n",
        "res = [i for i in res if i.strip()]\n",
        "print(len(res))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 word2ID\n",
        "all_words = sorted(set(res))\n",
        "all_words.extend(['<|endoftext|>','<|unk|>'])\n",
        "v_size = len(all_words)\n",
        "v_size\n",
        "v = {token:integer for token,integer in enumerate(all_words)}\n",
        "# for k,i in v.items():\n",
        "#   print(k,i)\n",
        "#   if k>=50:\n",
        "#     break\n",
        "len(v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgo8v9H2NPJO",
        "outputId": "90356419-95cc-42b1-b0cf-7345a3d61f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class TokenV1:\n",
        "#   def __init__(self, vocab):\n",
        "#       self.i2s = vocab\n",
        "#       self.s2i = {s: i for i, s in self.i2s.items()}\n",
        "\n",
        "#   def encode(self,text):\n",
        "#     res = re.split(r'([.,;:?!\"()\\']|--|\\s)', text)\n",
        "#     w = [i.strip() for i in res if i.strip()]\n",
        "#     ids = [self.s2i[i] for i in w]\n",
        "#     return ids\n",
        "\n",
        "#   def decode(self,ids):\n",
        "#     text = \" \".join([self.i2s[i] for i in ids])\n",
        "#     # text=re.sub(r'\\s+([,.?!\"()\\'])', r'\\1',text)\n",
        "#     text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "#     return text\n",
        "\n",
        "# test = \"\"\"\"Hello It's the last he painted, you know,\"\n",
        "#            Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "# tokenizer = TokenV1(v)\n",
        "# enc = tokenizer.encode(test)\n",
        "# tokenizer.decode(enc)\n",
        "\n",
        "# The problem is we cannot encode the unknown word in the vocab\n"
      ],
      "metadata": {
        "id": "BlQV07NQOpjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenV2:\n",
        "  def __init__(self, vocab):\n",
        "      self.i2s = vocab\n",
        "      self.s2i = {s: i for i, s in self.i2s.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    res = re.split(r'([.,;:?!\"()\\']|--|\\s)', text)\n",
        "    w = [i.strip() for i in res if i.strip()]\n",
        "    ids = []\n",
        "    for i in w:\n",
        "      if i in self.s2i:\n",
        "        ids.append(self.s2i[i])\n",
        "      else:\n",
        "        ids.append(self.s2i['<|unk|>'])\n",
        "    return ids\n",
        "\n",
        "  def decode(self,ids):\n",
        "    text = \" \".join([self.i2s[i] for i in ids])\n",
        "    # text=re.sub(r'\\s+([,.?!\"()\\'])', r'\\1',text)\n",
        "    text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n",
        "\n",
        "test = \"\"\"\"Hello It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "tokenizer = TokenV2(v)\n",
        "enc = tokenizer.encode(test)\n",
        "tokenizer.decode(enc)\n",
        "\n",
        "# The problem is we cannot encode the unknown word in the vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjMzK1ClYCi1",
        "outputId": "d352ecbe-04b4-47a8-936c-2a5f0acc9a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" <|unk|> It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TokenV2(v)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "enc = tokenizer.encode(text)\n",
        "tokenizer.decode(enc)\n",
        "# print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7gx5f6hmOuk",
        "outputId": "5a5e4c9b-e8aa-4bbe-cac2-4c7b301beac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Byte Pair Encoding BPE\n",
        "# !pip install tiktoken\n",
        "import tiktoken\n",
        "tk = tiktoken.get_encoding('gpt2')\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "text1 = (\n",
        "    \"<|endoftext|>\"\n",
        ")\n",
        "\n",
        "text2 = (\n",
        "    \"Helloa \" )\n",
        "\n",
        "\n",
        "# id = tk.encode(text1,disallowed_special=())\n",
        "id = tk.encode(text2,allowed_special={\"<|endoftext|>\"})\n",
        "txt = tk.decode(id)\n",
        "id\n",
        "tk.decode([0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPE-FL-Sm79m",
        "outputId": "bd74797a-99db-4392-ff65-a61e7b1b7934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('the-verdict.txt') as f:\n",
        "  raw = f.read()\n",
        "r_enc = tk.encode(raw)\n",
        "r_txt = tk.decode(r_enc)\n",
        "r_sample = r_enc[50:]\n",
        "len(r_sample)\n",
        "\n",
        "c_size = 4\n",
        "# x = r_sample[:c_size]\n",
        "# # y = r_sample[1:c_size+1]\n",
        "# # print(x)\n",
        "# # print('    ',y)\n",
        "\n",
        "for i in range(1,c_size+1):\n",
        "  c = r_sample[:i]\n",
        "  desired = r_sample[i]\n",
        "  # print(c,'--->',desired)\n",
        "  print(tk.decode(c),'--->',tk.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQDUGKh5vfp1",
        "outputId": "ee59136f-8385-4e99-df4f-e0d73f672e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and --->  established\n",
            " and established --->  himself\n",
            " and established himself --->  in\n",
            " and established himself in --->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "class GDataV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,window_size,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0,len(token_ids)-window_size,stride):\n",
        "      input_chunk = token_ids[i:i+window_size]\n",
        "      target_chunk = token_ids[i+1:i+window_size+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    x = self.input_ids[index]\n",
        "    y = self.target_ids[index]\n",
        "    return x,y\n",
        "\n",
        "\n",
        "# d = GDataV1(raw,tk,4,10)\n",
        "# d[10]\n",
        "len(tk.encode(raw))\n",
        "\n",
        "\n",
        "def dataloaderV1(txt,batch_size,window_size,stride,shuffle=True,drop_last=True,num_workers=0):\n",
        "  dataset = GDataV1(txt,tk,window_size,stride)\n",
        "  dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n",
        "dl = dataloaderV1(raw,batch_size=8,window_size=4,stride=4,shuffle=False)\n",
        "data_iter = iter(dl)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wO-_dJgF_TN",
        "outputId": "60b78302-8d84-4a59-a678-4d0d3bb733d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding\n",
        "# e.g.\n",
        "# input_id = torch.tensor\n",
        "torch.manual_seed(123)\n",
        "embedding = torch.nn.Embedding(len(v),5)\n",
        "embedding\n",
        "\n",
        "v_size=6\n",
        "out_dim=3\n",
        "torch.manual_seed(123)\n",
        "eb = torch.nn.Embedding(v_size,out_dim)\n",
        "print(eb.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2bZSewuSboN",
        "outputId": "1a0055e2-0f2b-4a11-b57c-2a28cb07fc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rlscdCM3EpEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 3**"
      ],
      "metadata": {
        "id": "QuA8LjeDEyjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "j3U3ybiZUfch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "1_-WxeIwE4Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "# att_score = torch.empty(inputs.shape[0])\n",
        "# query = inputs[1]\n",
        "# for i,j in enumerate(inputs):\n",
        "#   att_score[i]=torch.dot(query,j)\n",
        "# att_score = torch.softmax(att_score,dim=0)\n",
        "# att_score\n",
        "\n",
        "# cotxt_vec2 =torch.zeros(query.shape)\n",
        "# cotxt_vec2\n",
        "# for i,j in enumerate(inputs):\n",
        "#   cotxt_vec2 += att_score[i]*j\n",
        "# cotxt_vec2\n"
      ],
      "metadata": {
        "id": "hGDN4THzSDQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# at_ss = torch.zeros(inputs.shape[0],inputs.shape[0])\n",
        "# for i,x_i in enumerate(inputs):\n",
        "#   query = x_i\n",
        "#   for j,x_j in enumerate(inputs):\n",
        "#     at_ss[i][j] = torch.dot(x_i,x_j)\n",
        "# at_ss\n",
        "\n",
        "at_scores = inputs @ inputs.T\n",
        "at_scores.shape\n",
        "at_weights = torch.softmax(at_scores,dim=1)\n",
        "at_weights\n",
        "\n",
        "at_vec = at_weights @ inputs\n",
        "at_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TJhiVk6Y5DG",
        "outputId": "49bbb588-9ee3-486d-c811-1a479c1bc810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# qkv\n",
        "#sample\n",
        "# x2 = inputs[1]\n",
        "# x2\n",
        "# d_in = x2.shape[0]\n",
        "# d_out = 2\n",
        "# torch.manual_seed(123)\n",
        "# wq = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "# wk = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "# wv = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "# q2 = x2 @ wq\n",
        "# k2 = x2 @ wk\n",
        "# v2 = x2 @ wv\n",
        "# q2\n"
      ],
      "metadata": {
        "id": "SFnTRBX6ymfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import isqrt\n",
        "#qkv\n",
        "d_in = inputs.shape[1]\n",
        "d_in\n",
        "d_out = 2\n",
        "torch.manual_seed(123)\n",
        "wq = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "wk = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "wv = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "q = inputs @ wq\n",
        "k = inputs @ wk\n",
        "v = inputs @ wv\n",
        "\n",
        "q2 = q[1]\n",
        "k2 = k[1]\n",
        "at_sc22 = torch.dot(q2,k2)\n",
        "# at_sc = q @ k.T\n",
        "# at_sc2\n",
        "at_sc2 = q2 @ k.T\n",
        "at_sc2\n",
        "dk = k.shape[-1]\n",
        "at_weights = torch.softmax(at_sc2/dk**0.5,dim=-1)\n",
        "# at_weights = torch.softmax(at_sc2,dim=-1)\n",
        "\n",
        "at_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmvLI2-t1i4d",
        "outputId": "12d2b0c3-a84a-4888-f2ba-aa150d664680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_in = inputs.shape[1]\n",
        "d_in\n",
        "d_out = 2\n",
        "torch.manual_seed(123)\n",
        "wq = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "wk = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "wv = torch.nn.Parameter(torch.rand(d_in,d_out))\n",
        "q = inputs @ wq\n",
        "k = inputs @ wk\n",
        "v = inputs @ wv\n",
        "\n",
        "at_sc = q @ k.T\n",
        "at_w = torch.softmax(at_sc/k.shape[-1]**0.5,dim=-1)\n",
        "cotxt_vec = at_w @ v\n",
        "cotxt_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DYQ7Baj-LJF",
        "outputId": "ddf629b7-5255-4641-b66b-55435d7bc1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2996, 0.8053],\n",
              "        [0.3061, 0.8210],\n",
              "        [0.3058, 0.8203],\n",
              "        [0.2948, 0.7939],\n",
              "        [0.2927, 0.7891],\n",
              "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUWtWtzJXfHq",
        "outputId": "8f09f190-ac5b-4e02-dd4b-fa2d05239a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4300, 0.1500, 0.8900],\n",
              "        [0.5500, 0.8700, 0.6600],\n",
              "        [0.5700, 0.8500, 0.6400],\n",
              "        [0.2200, 0.5800, 0.3300],\n",
              "        [0.7700, 0.2500, 0.1000],\n",
              "        [0.0500, 0.8000, 0.5500]])"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mCuSgbntu4Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "Pllg5RUxvOA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class selfattentionV2 (nn.Module):\n",
        "  def __init__(self,din,dout):\n",
        "    super().__init__()\n",
        "    self.wq = nn.Linear(din,dout,bias=False)\n",
        "    self.wk = nn.Linear(din,dout,bias=False)\n",
        "    self.wv = nn.Linear(din,dout,bias=False)\n",
        "  def forward(self,x):\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "    att_score = q @ k.T\n",
        "    att_weights = torch.softmax(att_score/k.shape[-1]**0.5, dim=-1)\n",
        "    ct_vec = att_weights @ v\n",
        "    return ct_vec"
      ],
      "metadata": {
        "id": "JjRnZXhwvSb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class selfattentionV1 (nn.Module):\n",
        "  def __init__(self,din,dout):\n",
        "    super().__init__()\n",
        "    self.wq = nn.Parameter(torch.rand(din,dout))\n",
        "    self.wk = nn.Parameter(torch.rand(din,dout))\n",
        "    self.wv = nn.Parameter(torch.rand(din,dout))\n",
        "\n",
        "  def forward(self,x):\n",
        "    q = x @ self.wq\n",
        "    k = x @ self.wk\n",
        "    v = x @ self.wv\n",
        "    score = q @ k.T\n",
        "    att_weights = torch.softmax(score/k.shape[-1]**0.5,dim=1)\n",
        "    context_vector = att_weights @ v\n",
        "    return context_vector\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wxGLcY1PT2cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "a = selfattentionV2(3,2) #a/V2\n",
        "b = selfattentionV1(3,2) #b/V1\n",
        "\n",
        "b.wq = nn.Parameter(a.wq.weight.T)\n",
        "b.wk = nn.Parameter(a.wk.weight.T)\n",
        "b.wv = nn.Parameter(a.wv.weight.T)\n",
        "b.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTXeMwf50A2A",
        "outputId": "e2b4c929-da6a-45e9-e67c-083564a5d882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0739,  0.0713],\n",
              "        [-0.0748,  0.0703],\n",
              "        [-0.0749,  0.0702],\n",
              "        [-0.0760,  0.0685],\n",
              "        [-0.0763,  0.0679],\n",
              "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hiding future words\n",
        "q = a.wq(inputs)\n",
        "k = a.wk(inputs)\n",
        "v = a.wv(inputs)\n",
        "sc =  (q@k.T)\n",
        "aw = torch.softmax(sc/k.shape[-1]**0.5,dim=-1)\n",
        "aw\n",
        "\n",
        "mask_sample = torch.tril(torch.ones(aw.shape[0],aw.shape[0]),diagonal=0)\n",
        "aw *=mask_sample\n",
        "row_sum = aw.sum(dim=-1,keepdim=True)\n",
        "row_sum.shape\n",
        "aw.shape\n",
        "aw = aw/row_sum\n",
        "aw\n",
        "# mask_sample\n",
        "\n",
        "# # a simpler way\n",
        "q = a.wq(inputs)\n",
        "k = a.wk(inputs)\n",
        "v = a.wv(inputs)\n",
        "sc =  (q@ k.T)\n",
        "mask = torch.triu(torch.ones(sc.shape[0],sc.shape[0]),diagonal=1)\n",
        "sc = sc.masked_fill(mask.bool(),-torch.inf)\n",
        "# maskd\n",
        "aw = torch.softmax(sc/k.shape[-1]**0.5,dim=-1)\n",
        "aw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qtKx2vEDA_f",
        "outputId": "4b698977-f466-4f6e-e264-acad3da6a856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
              "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
              "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout\n",
        "torch.manual_seed(123)\n",
        "example = torch.ones((6,6))\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "dropout(aw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-khnCOgfKvWj",
        "outputId": "3a3c3345-324b-4767-d86b-602ac843d898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs\n",
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFW_t3OaSNip",
        "outputId": "de0dacb1-3c74-48f0-a12f-a1a49a3697ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self,din,dout,dropout,context_length):\n",
        "    super().__init__()\n",
        "    self.wq = nn.Linear(din,dout,bias=False)\n",
        "    self.wk = nn.Linear(din,dout,bias=False)\n",
        "    self.wv = nn.Linear(din,dout,bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_token,d_in = x.shape\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "    att_score = q @ k.transpose(1,2)\n",
        "    att_score.masked_fill_(self.mask.bool()[:num_token, :num_token],-torch.inf)\n",
        "    aw = torch.softmax(att_score/k.shape[-1]**0.5,dim=-1)\n",
        "    aw = self.dropout(aw)\n",
        "    cont_vec = aw @ v\n",
        "\n",
        "    return cont_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "test = CausalAttention(3,2,0.5,6)\n",
        "# test.forward(batch)\n",
        "test(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbYN-FLOq8NP",
        "outputId": "d8571223-a9f7-4f9b-d0e6-fc6f753ea7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000,  0.0000],\n",
              "         [-0.4368,  0.2142],\n",
              "         [-0.7751,  0.0077],\n",
              "         [-0.9140, -0.2769],\n",
              "         [ 0.0000,  0.0000],\n",
              "         [-0.6906, -0.0974]],\n",
              "\n",
              "        [[-0.9038,  0.4432],\n",
              "         [ 0.0000,  0.0000],\n",
              "         [-0.2883,  0.1414],\n",
              "         [-0.9140, -0.2769],\n",
              "         [-0.4416, -0.1410],\n",
              "         [-0.5272, -0.1706]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multihead\n",
        "class Multihead (nn.Module):\n",
        "  def __init__(self,din,dout,dropout,context_length,num_head):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([CausalAttention(din,dout,dropout,context_length) for _ in range(num_head)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.cat([head(x) for head in self.heads],dim=2)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "mh = Multihead(3,1,0,batch.shape[1],2)\n",
        "mh(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5IwOsgHvJsD",
        "outputId": "58092ba7-e284-40cb-a2a5-a4117eac4dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5740,  0.2216],\n",
              "         [-0.7320,  0.0155],\n",
              "         [-0.7774, -0.0546],\n",
              "         [-0.6979, -0.0817],\n",
              "         [-0.6538, -0.0957],\n",
              "         [-0.6424, -0.1065]],\n",
              "\n",
              "        [[-0.5740,  0.2216],\n",
              "         [-0.7320,  0.0155],\n",
              "         [-0.7774, -0.0546],\n",
              "         [-0.6979, -0.0817],\n",
              "         [-0.6538, -0.0957],\n",
              "         [-0.6424, -0.1065]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "batch_size, context_length, d_in\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "gpt2mha = MultiHeadAttention(768, 768, 1024, 0.0, num_heads=12)\n",
        "batch = torch.randn(2, 1024, 768)\n",
        "context_vecs = gpt2mha(batch)\n",
        "\n",
        "# print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Ih-FeI76sv",
        "outputId": "22e88177-9dd9-461f-e755-ba4bd923b5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 1024, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "  def __init__(self,din,dout,num_heads,dropout,context_length):\n",
        "    super().__init__()\n",
        "    assert(dout % num_heads == 0),'dout & num_heads should be divisiable'\n",
        "    self.head_dim = dout//num_heads\n",
        "    self.num_heads= num_heads\n",
        "    self.wq = nn.Linear(din,dout,bias=False)\n",
        "    self.wk = nn.Linear(din,dout,bias=False)\n",
        "    self.wv = nn.Linear(din,dout,bias=False)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),1))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out_proj = nn.Linear(dout,dout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,seq_len,din = x.shape\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "\n",
        "    q = q.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "    k = k.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "    v = v.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "\n",
        "    q.transpose_(1,2)\n",
        "    k.transpose_(1,2)\n",
        "    v.transpose_(1,2)\n",
        "\n",
        "    attn_score = q @ k.transpose(2,3)\n",
        "    attn_score = attn_score.masked_fill(self.mask.bool()[:seq_len,:seq_len],-torch.inf)\n",
        "    attn_weights = torch.softmax(attn_score/k.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    con_vec = attn_weights @ v\n",
        "    con_vec.transpose_(1,2)\n",
        "    con_vec = con_vec.reshape(b,seq_len,dout)\n",
        "    con_vec = self.out_proj(con_vec)\n",
        "\n",
        "\n",
        "\n",
        "    return con_vec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "batch = torch.stack((inputs,inputs))\n",
        "b,cl,din = batch.shape\n",
        "torch.manual_seed(123)\n",
        "dout = 2\n",
        "# test = MultiHeadAttention(din,dout,num_heads=2,dropout=0,context_length=cl) #context_length != seq_len\n",
        "test = MHA(din,dout,num_heads=2,dropout=0,context_length=cl) #context_length != seq_len\n",
        "gpt2mha = MHA(768,768,num_heads=12,dropout=0,context_length=1024)\n",
        "# gpt2mha.\n",
        "# test(batch).shape\n",
        "# test(batch)"
      ],
      "metadata": {
        "id": "62vFkyffmO2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## useful test"
      ],
      "metadata": {
        "id": "F_gEUg-y77Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## useful test\n",
        "a = torch.tensor(([1,2,3],[4,5,6],[7,8,9],[10,11,12]))\n",
        "b = torch.tensor(([81,82,83],[84,85,86],[87,88,89],[90,91,92]))\n",
        "c = torch.ones(5,5)\n",
        "torch.triu(c,1)\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "batch = torch.stack((inputs,inputs))\n",
        "batch\n",
        "\n",
        "a = nn.Embedding(10,3)\n",
        "a.weight.shape\n",
        "b = nn.Embedding(6,3)\n",
        "b.weight.shape\n",
        "\n",
        "# torch.stack((a,b),dim=2)\n",
        "# torch.cat((a,b),dim=1)"
      ],
      "metadata": {
        "id": "dFRHLp7d6ZPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ae5369-8b8e-497a-ecf2-2916e88c7641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tok,pos embedding\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "tok_emb = nn.Embedding(10, 3)\n",
        "pos_emb = nn.Embedding(6, 3)\n",
        "\n",
        "in_idx = torch.tensor([\n",
        "    [2, 4, 1, 5, 6, 3],\n",
        "    [0, 3, 7, 2, 1, 4]\n",
        "])\n",
        "\n",
        "tok_embeds = tok_emb(in_idx) #[2,6,3]\n",
        "\n",
        "seq_len = in_idx.shape[1]\n",
        "positions = torch.arange(seq_len)  # [0,1,2,3,4,5]\n",
        "pos_embeds = pos_emb(positions)   # shape: [6, 3]\n",
        "tok_embeds.shape\n",
        "# pos_embeds = pos_embeds.unsqueeze(0).expand(in_idx.shape[0], -1, -1)  # shape: [2, 6, 3]\n",
        "tok_embeds+pos_embeds\n",
        "\n",
        "# # 最终嵌入 = token_embedding + position_embedding\n",
        "# final_embeds = tok_embeds + pos_embeds  # shape: [2, 6, 3]\n",
        "\n",
        "# print(\"tok_embeds:\\n\", tok_embeds)\n",
        "# print(\"pos_embeds:\\n\", pos_embeds)\n",
        "# print(\"final_embeds:\\n\", final_embeds)\n"
      ],
      "metadata": {
        "id": "sbT_mIHSQ__Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf64b960-4b94-4c44-d1eb-398ff4198781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[    -0.8307,     -2.4864,     -0.8476],\n",
              "         [    -2.6106,     -0.2579,      1.1147],\n",
              "         [     0.6856,     -1.3279,      1.9351],\n",
              "         [    -0.8320,      0.1845,      0.9229],\n",
              "         [    -0.4934,      0.0182,     -0.7258],\n",
              "         [    -2.4332,     -0.4354,     -0.9643]],\n",
              "\n",
              "        [[     1.1640,     -0.4209,     -1.2259],\n",
              "         [    -1.8369,      1.5507,      2.2071],\n",
              "         [     0.8147,     -1.9544,      1.6168],\n",
              "         [     0.0485,     -2.8453,     -0.0005],\n",
              "         [    -0.0069,     -1.2586,     -0.4248],\n",
              "         [    -3.2069,     -2.2441,     -2.0568]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## play with dataloader (have fun)"
      ],
      "metadata": {
        "id": "q3fpNSvgbbiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "with open('the-verdict.txt') as f:\n",
        "  raw = f.read()\n",
        "\n",
        "tk = tiktoken.get_encoding('gpt2')\n",
        "# r_enc = tk.encode(raw)\n",
        "# r_dec = tk.decode(r_enc)\n",
        "# r_sample = r_enc[50:]\n",
        "# c_size = 4\n",
        "# for i in range(1,c_size+1):\n",
        "#   c_txt = tk.decode(r_sample[:i])\n",
        "#   c_d = tk.decode(r_sample[i:i+1])\n",
        "#   print(f'{c_txt} ---> {c_d}')\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "class testData(Dataset):\n",
        "  def __init__(self,input,window_size,stride,token=tk):\n",
        "    r_enc = token.encode(input)\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(0,len(r_enc)-window_size,stride):\n",
        "      cur_x = r_enc[i:i+window_size]\n",
        "      cur_y = r_enc[i+1:i+window_size+1]\n",
        "      x.append(cur_x)\n",
        "      y.append(cur_y)\n",
        "    self.rx =torch.tensor(x)\n",
        "    self.ry =torch.tensor(y)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.rx)\n",
        "\n",
        "  def __getitem__(self,ind):\n",
        "    return self.rx[ind],self.ry[ind]\n",
        "\n",
        "\n",
        "def testloader(txt,bs,dl,shuffle,window_size,stride):\n",
        "  dataset = testData(txt,window_size=window_size,stride=stride)\n",
        "  tloader = DataLoader(dataset=dataset,batch_size=bs,drop_last=dl,shuffle=shuffle)\n",
        "  return tloader\n",
        "a = testloader(raw,bs=6,dl=True,shuffle=True,stride=4,window_size=4)\n",
        "a = iter(a)\n",
        "next(a)"
      ],
      "metadata": {
        "id": "v_SM46J4ZVbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8d3da8-c8dc-4982-876f-0f906d13d7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 1364,  2157,   438, 13893],\n",
              "         [ 2126,   373,   284,   423],\n",
              "         [ 1327,   284,  5879,   326],\n",
              "         [  470,  6842,   407,   284],\n",
              "         [  286,   257, 50085,   438],\n",
              "         [   11,   262,  7888,  7586]]),\n",
              " tensor([[ 2157,   438, 13893,   339],\n",
              "         [  373,   284,   423,   683],\n",
              "         [  284,  5879,   326,   339],\n",
              "         [ 6842,   407,   284,   423],\n",
              "         [  257, 50085,   438,   272],\n",
              "         [  262,  7888,  7586,  9813]])]"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test singlehead/multihead stack,/multihead"
      ],
      "metadata": {
        "id": "-FggZtqoulLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Singlehead"
      ],
      "metadata": {
        "id": "lzLDjguwwjW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n"
      ],
      "metadata": {
        "id": "xWXlKdX9uy2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.unsqueeze_(0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPUa4FdgwLB-",
        "outputId": "d0278921-f272-4098-e107-0720f1df3570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch = torch.stack((inputs,inputs))\n",
        "batch.shape\n",
        "batch.squeeze_().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTiHGq3GvAkH",
        "outputId": "2b2749b8-4f4e-492c-fb55-c22b2d657d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape, batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVdne_eHwOXY",
        "outputId": "4283d932-04e9-4cca-d6cd-4fd91b8b604d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 6, 3]), torch.Size([2, 6, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Causal(nn.Module):\n",
        "  def __init__(self,din,dout,context_length,dropout):\n",
        "    super().__init__()\n",
        "    self.wq = nn.Linear(din,dout)\n",
        "    self.wk = nn.Linear(din,dout)\n",
        "    self.wv = nn.Linear(din,dout)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones([context_length,context_length]),diagonal=1))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_token,din = x.shape\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "\n",
        "    att_score = q @ k.transpose(1,2)\n",
        "    att_score.masked_fill_(mask.bool()[:num_token, :num_token],-torch.inf)\n",
        "    att_weights = torch.softmax(att_score/k.shape[-1]**0.5,dim=-1)\n",
        "    att_weights = self.dropout(att_weights)\n",
        "    con_vec = att_weights @ v\n",
        "    return con_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "test = Causal(3,2,6,0)\n",
        "test(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "086MBAD8uy_i",
        "outputId": "30863fec-19a3-4119-8686-a9976cd9d906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0960,  0.7940],\n",
              "         [ 0.0285,  0.9387],\n",
              "         [ 0.0657,  0.9850],\n",
              "         [ 0.1062,  0.9604],\n",
              "         [ 0.0659,  0.9308],\n",
              "         [ 0.1188,  0.9375]],\n",
              "\n",
              "        [[-0.0960,  0.7940],\n",
              "         [ 0.0285,  0.9387],\n",
              "         [ 0.0657,  0.9850],\n",
              "         [ 0.1062,  0.9604],\n",
              "         [ 0.0659,  0.9308],\n",
              "         [ 0.1188,  0.9375]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multihead stack"
      ],
      "metadata": {
        "id": "lal3UJmSwrS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "batch.shape\n",
        "\n",
        "class MHAS(nn.Module):\n",
        "  def __init__(self,num_head,din,dout,dropout,context_length):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([CausalAttention(din,dout,dropout,context_length) for _ in range(num_head)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.cat([head(x) for head in self.heads],dim=2)\n",
        "\n",
        "m = MHAS(din=3,dout=2,dropout=0.5,context_length=6,num_head=2)\n",
        "m(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k14NLoynuzCO",
        "outputId": "f362c15b-9112-4953-97fb-8d555b956c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.1934, -0.5507,  0.0000,  0.0000],\n",
              "         [-0.5803, -0.2678, -1.1862, -0.1992],\n",
              "         [-0.9239, -0.3703, -0.7693, -0.1186],\n",
              "         [ 0.0000,  0.0000, -0.7991, -0.1040],\n",
              "         [-0.6868, -0.2290, -0.5946,  0.0013],\n",
              "         [-0.6100, -0.1930, -0.5601, -0.0263]],\n",
              "\n",
              "        [[ 0.0000,  0.0000, -0.8607, -0.3983],\n",
              "         [-0.5803, -0.2678, -0.7728, -0.0079],\n",
              "         [-0.8456, -0.3650, -0.7807, -0.1313],\n",
              "         [-0.8593, -0.3549, -0.9723, -0.0108],\n",
              "         [-0.3192, -0.0747, -0.9418, -0.0880],\n",
              "         [-0.4932, -0.1517, -0.3428, -0.1308]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multihead"
      ],
      "metadata": {
        "id": "i7Vyz9pO1YNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "batch = torch.stack((inputs,inputs),dim=0)\n",
        "batch.shape\n",
        "\n",
        "class MHAP(nn.Module):\n",
        "  def __init__(self,num_head,din,dout,dropout,context_length):\n",
        "    assert(dout%num_head == 0), \"dout%num_head == 0\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.context_length =context_length\n",
        "    self.head_dim = dout//num_head\n",
        "    self.dout = dout\n",
        "    self.din = din\n",
        "    self.num_head = num_head\n",
        "\n",
        "\n",
        "    self.wq = nn.Linear(din,dout)\n",
        "    self.wk = nn.Linear(din,dout)\n",
        "    self.wv = nn.Linear(din,dout)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones([context_length,context_length]),diagonal=1))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self,x):\n",
        "    b,num_token,din = x.shape\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "\n",
        "\n",
        "    q = q.view(b,num_token,self.num_head,self.head_dim)\n",
        "    k = q.view(b,num_token,self.num_head,self.head_dim)\n",
        "    v = q.view(b,num_token,self.num_head,self.head_dim)\n",
        "\n",
        "\n",
        "    q.transpose_(1,2)\n",
        "    k.transpose_(1,2)\n",
        "    v.transpose_(1,2)\n",
        "\n",
        "    attn_score = q @ k.transpose(2,3)\n",
        "    attn_score = attn_score.masked_fill(self.mask.bool()[:num_token,:num_token],-torch.inf)\n",
        "    attn_weights = torch.softmax(attn_score/k.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    con_vec = attn_weights @ v\n",
        "    con_vec.transpose_(1,2)\n",
        "    con_vec = con_vec.contiguous().view(b,num_token,self.dout)\n",
        "    return con_vec\n",
        "\n",
        "t = MHAP(din=3,dout=2,dropout=0.5,context_length=6,num_head=2)\n",
        "t(batch).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n65xEPUN1kdX",
        "outputId": "d7ab7326-3d05-46c7-bec7-17d8cee1b35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rCFkBFmQ1kQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 4**"
      ],
      "metadata": {
        "id": "b-gimTPSWDgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "oS4tbixjloMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_length\":1024,\n",
        "    \"emb_dim\":768,\n",
        "    \"n_heads\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"dropout_rate\":0.1,\n",
        "    \"qkv_bias\":False}\n"
      ],
      "metadata": {
        "id": "TYF7sCV4WN0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyGPT(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "    self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "    self.drop_emb =nn.Dropout(cfg['dropout_rate'])\n",
        "    self.trf_blocks = nn.Sequential(*[DummyTB(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.final_norm=DummyLayerNorm(cfg)\n",
        "    self.out_head = nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len = in_idx.shape\n",
        "    tok_embeds=self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "class DummyTB(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return x\n",
        "\n",
        "\n",
        "# class DummyLayerNorm(nn.Module):\n",
        "#   def __init__(self,cfg):\n",
        "#     super().__init__()\n",
        "#   def forward(self,x):\n",
        "#     return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones((cfg['emb_dim'])))\n",
        "    self.shift = nn.Parameter(torch.zeros((cfg['emb_dim'])))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(keepdim=True,dim=-1)\n",
        "    var = x.var(keepdim=True,dim=-1,unbiased=False)\n",
        "    out_norm = (x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*out_norm + self.shift\n",
        "\n",
        "import tiktoken\n",
        "tk = tiktoken.get_encoding('gpt2')\n",
        "batch = []\n",
        "txt1 = 'Every effort moves you'\n",
        "txt2 = 'Every day holds a'\n",
        "batch.append(torch.tensor(tk.encode(txt1)))\n",
        "batch.append(torch.tensor(tk.encode(txt2)))\n",
        "batch = torch.stack(batch,dim=0)\n",
        "batch.shape\n",
        "torch.manual_seed(123)\n",
        "model = DummyGPT(CFG)\n",
        "logits = model(batch)\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fbfUmZllLdf",
        "outputId": "9542a703-c8df-4304-e548-0628e4454c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FjvCCQMr6i6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Norm"
      ],
      "metadata": {
        "id": "196L6LqXIZES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Norm Example\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "torch.manual_seed(123)\n",
        "batch = torch.randn((2,5))\n",
        "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
        "out= layer(batch)\n",
        "out\n",
        "mean = out.mean(keepdim=True,dim=-1)\n",
        "var = out.var(keepdim=True,dim=-1)\n",
        "out_norm = (out-mean)/torch.sqrt(var)\n",
        "out_norm.mean(keepdim=True,dim=-1)\n",
        "# out_norm.var(keepdim=True,dim=-1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90edWPOPlLk2",
        "outputId": "486d1e9b-654a-44b0-de5a-6e27f24788ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0.0000],\n",
              "        [    0.0000]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3k8mgSQ2Cdn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Norm\n",
        "class NormLayer(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones((emb_dim)))\n",
        "    self.shift = nn.Parameter(torch.zeros((emb_dim)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(keepdim=True,dim=-1)\n",
        "    var = x.var(keepdim=True,dim=-1,unbiased=False)\n",
        "    out_norm = (x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*out_norm + self.shift\n",
        "\n",
        "\n",
        "ln = NormLayer(5)\n",
        "out_ln = ln(batch)\n",
        "out_ln,out_ln.mean(keepdim=True,dim=-1),out_ln.var(keepdim=True,dim=-1,unbiased=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeY9FWh0lLnT",
        "outputId": "7bc14d3c-8ca0-4f76-add5-f5465de079f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
              "         [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>),\n",
              " tensor([[    -0.0000],\n",
              "         [     0.0000]], grad_fn=<MeanBackward1>),\n",
              " tensor([[1.0000],\n",
              "         [1.0000]], grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5rmDtZlQlLq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pnq1VgHgWFGC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aa1Wtv-YIfI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "test = torch.tensor([0.43, 0.15, 0.89])\n",
        "a = GELU()\n",
        "a(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBn4j5kamnfj",
        "outputId": "94663697-4c83-4b0e-fa5d-8cb1386fed75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2865, 0.0839, 0.7237])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "gelu,relu = GELU(), nn.ReLU()\n",
        "\n",
        "x = torch.linspace(-3,3,100)\n",
        "y_gelu,y_relu = gelu(x),relu(x)\n",
        "plt.figure(figsize=(8, 3))\n",
        "# y_gelu.shape\n",
        "for i,(y,label) in enumerate(zip([y_gelu,y_relu],['GELU','RELU']),1):\n",
        "  plt.subplot(1,2,i)\n",
        "  plt.plot(x,y)\n",
        "  plt.title(f'{label} Activation')\n",
        "  plt.grid()\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "2g4K8UwPkFT6",
        "outputId": "63b3a161-503b-42a0-9623-82b1c3b9a51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu(torch.tensor([-1,-1,-1]))\n",
        "gelu(torch.tensor([-2,-2,-2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBhPrFuWmdAC",
        "outputId": "3a2c3f1c-feca-4274-9715-98267363c3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0454, -0.0454, -0.0454])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Sequential(nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']) ,GELU(),nn.Linear(4* cfg['emb_dim'],cfg['emb_dim']))\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layer(x)\n",
        "\n",
        "test = torch.randn([2,3,768])\n",
        "\n",
        "tc = FeedForward(CFG)\n",
        "tc(test).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atsUaBJ7mGoU",
        "outputId": "6b61999e-faee-41c5-96ef-e2afd735b57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# neural shortcut\n",
        "class ExampleDNN(nn.Module):\n",
        "  def __init__(self,layer_size,use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([nn.Sequential(nn.Linear(layer_size[0],layer_size[1]),GELU()),\n",
        "                            nn.Sequential(nn.Linear(layer_size[1],layer_size[2]),GELU()),\n",
        "                            nn.Sequential(nn.Linear(layer_size[2],layer_size[3]),GELU()),\n",
        "                            nn.Sequential(nn.Linear(layer_size[3],layer_size[4]),GELU()),\n",
        "                            nn.Sequential(nn.Linear(layer_size[4],layer_size[5]),GELU())\n",
        "                            ])\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(x)\n",
        "      if self.use_shortcut == True:\n",
        "        x = x+layer_output\n",
        "      else:\n",
        "        x= layer_output\n",
        "    return x\n",
        "\n",
        "layer_size = [3,3,3,3,3,1]\n",
        "sample_input = torch.tensor([[1.,0.,1.]])\n",
        "# sample_input.dtype\n",
        "torch.manual_seed(123)\n",
        "Enoshort = ExampleDNN(layer_size,False)\n",
        "Eshort = ExampleDNN(layer_size,True)\n",
        "Enoshort(sample_input)\n",
        "\n",
        "\n",
        "def printgradient(model,x):\n",
        "  output = model(x)\n",
        "  target = torch.tensor([[0.]])\n",
        "\n",
        "  loss = torch.nn.MSELoss()\n",
        "  loss = loss(output,target)\n",
        "  loss.backward()\n",
        "\n",
        "  for name,param in model.named_parameters():\n",
        "    # if 'weight' in name:\n",
        "      print(name,param.grad.abs().mean().item())\n",
        "\n",
        "printgradient(Eshort,sample_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mbadIz48Szq",
        "outputId": "37633e81-88de-489b-bb22-256536ab61fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight 0.5671719908714294\n",
            "layers.0.0.bias 0.8507580757141113\n",
            "layers.1.0.weight 0.47776636481285095\n",
            "layers.1.0.bias 0.5876272916793823\n",
            "layers.2.0.weight 0.6719534397125244\n",
            "layers.2.0.bias 0.820436954498291\n",
            "layers.3.0.weight 0.7111383080482483\n",
            "layers.3.0.bias 0.7866613864898682\n",
            "layers.4.0.weight 0.7011792063713074\n",
            "layers.4.0.bias 0.4635065793991089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0715-Put all together"
      ],
      "metadata": {
        "id": "knYYyhuTyz5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Everything"
      ],
      "metadata": {
        "id": "kKkaXdbT6m0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_length\":1024,\n",
        "    \"emb_dim\":768,\n",
        "    \"n_heads\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"dropout_rate\":0.1,\n",
        "    \"qkv_bias\":False}\n",
        "\n",
        "class MHA(nn.Module):\n",
        "  def __init__(self,num_head,din,dout,dropout,context_length):\n",
        "    assert(dout%num_head == 0), \"dout%num_head == 0\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.context_length =context_length\n",
        "    self.head_dim = dout//num_head\n",
        "    self.dout = dout\n",
        "    self.din = din\n",
        "    self.num_head = num_head\n",
        "\n",
        "    self.out_proj = nn.Linear(dout, dout)\n",
        "    self.wq = nn.Linear(din,dout,bias=False)\n",
        "    self.wk = nn.Linear(din,dout,bias=False)\n",
        "    self.wv = nn.Linear(din,dout,bias=False)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones([context_length,context_length]),diagonal=1))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self,x):\n",
        "    b,num_token,din = x.shape\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "\n",
        "\n",
        "    q = q.view(b,num_token,self.num_head,self.head_dim)\n",
        "    k = k.view(b,num_token,self.num_head,self.head_dim)\n",
        "    v = v.view(b,num_token,self.num_head,self.head_dim)\n",
        "\n",
        "\n",
        "    q = q.transpose(1, 2)\n",
        "    k = k.transpose(1, 2)\n",
        "    v = v.transpose(1, 2)\n",
        "\n",
        "    attn_score = q @ k.transpose(2,3)\n",
        "    attn_score = attn_score.masked_fill(self.mask.bool()[:num_token,:num_token],-torch.inf)\n",
        "    attn_weights = torch.softmax(attn_score/k.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    con_vec = attn_weights @ v\n",
        "    con_vec.transpose_(1,2)\n",
        "    con_vec = con_vec.contiguous().view(b,num_token,self.dout)\n",
        "    con_vec = self.out_proj(con_vec)\n",
        "    return con_vec\n",
        "\n",
        "class NormLayer(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones((emb_dim)))\n",
        "    self.shift = nn.Parameter(torch.zeros((emb_dim)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(keepdim=True,dim=-1)\n",
        "    var = x.var(keepdim=True,dim=-1,unbiased=False)\n",
        "    out_norm = (x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*out_norm + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Sequential(nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']) ,GELU(),nn.Linear(4* cfg['emb_dim'],cfg['emb_dim']))\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layer(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att = MHA(cfg['n_heads'],cfg['emb_dim'],cfg['emb_dim'],cfg['dropout_rate'],cfg['context_length'])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = NormLayer(cfg['emb_dim'])\n",
        "    self.norm2 = NormLayer(cfg['emb_dim'])\n",
        "    self.drop_shotcut = nn.Dropout(cfg['dropout_rate'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shotcut(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shotcut(x)\n",
        "    x=x+shortcut\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "    self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "    self.drop_emb =nn.Dropout(cfg['dropout_rate'])\n",
        "    self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.final_norm=NormLayer(cfg['emb_dim'])\n",
        "    self.out_head = nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len = in_idx.shape\n",
        "    tok_embeds=self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPT(CFG)\n",
        "\n",
        "batch = []\n",
        "txt1 = 'Every effort moves you'\n",
        "txt2 = 'Every day holds a'\n",
        "batch.append(torch.tensor(tk.encode(txt1)))\n",
        "batch.append(torch.tensor(tk.encode(txt2)))\n",
        "batch = torch.stack(batch,dim=0)\n",
        "\n",
        "out = model(batch)\n",
        "# print(\"Input batch:\\n\", batch)\n",
        "# print(\"\\nOutput shape:\", out.shape)\n",
        "out.shape\n",
        "\n",
        "params = sum([p.numel() for p in model.parameters()])\n",
        "params - sum([p.numel() for p in model.out_head.parameters()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K00zSUnOYIUd",
        "outputId": "ac5a437c-1e4c-4e8c-db4e-0ee543ba2a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124412160"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.tok_emb.weight.shape\n",
        "model.out_head.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEKLu7PqblbU",
        "outputId": "17333c90-1bc9-45d4-9543-5737785cea4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YY-D6x0_blde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = NormLayer(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "batch = []\n",
        "txt1 = 'Every effort moves you'\n",
        "txt2 = 'Every day holds a'\n",
        "batch.append(torch.tensor(tk.encode(txt1)))\n",
        "batch.append(torch.tensor(tk.encode(txt2)))\n",
        "batch = torch.stack(batch,dim=0)\n",
        "\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "out.shape\n",
        "\n",
        "# params = sum([p.numel() for p in model.parameters()])\n",
        "# params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVsjVJvQYIXD",
        "outputId": "c96c12e1-de7f-487c-8f14-49e9b30b5fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test 0717, warp up"
      ],
      "metadata": {
        "id": "z056CnPlnCV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT-forward"
      ],
      "metadata": {
        "id": "jBLm4Uk7KIrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTforward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "    self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "    self.dropout = nn.Dropout(cfg['drop_rate'])\n",
        "    self.tb = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.finalNorm = NormLayer(cfg['emb_dim'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,seq_len = x.shape\n",
        "    t_emb = self.tok_emb(x)\n",
        "    p_emb = self.pos_emb(torch.arange(seq_len,device=x.device))\n",
        "    xx = t_emb+p_emb\n",
        "    xx = self.dropout(xx)\n",
        "    for i in self.tb:\n",
        "      xx = i(xx)\n",
        "    xx = self.finalNorm(xx)\n",
        "\n",
        "    return xx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ndyqCfJpKMse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MHA\n"
      ],
      "metadata": {
        "id": "hiK9EQUgJGaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "    # def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    def __init__(self, d_in, d_out, num_heads,context_length,dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.head_dim = d_out//num_heads\n",
        "        self.wq = nn.Linear(d_in,d_out,qkv_bias)\n",
        "        self.wk = nn.Linear(d_in,d_out,qkv_bias)\n",
        "        self.wv = nn.Linear(d_in,d_out,qkv_bias)\n",
        "        self.num_heads = num_heads\n",
        "        self.register_buffer('mask',torch.triu(torch.ones((context_length,context_length)),1))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "      b,seq_len,emb_dim = x.shape\n",
        "      q = self.wq(x)\n",
        "      k = self.wk(x)\n",
        "      v = self.wv(x)\n",
        "\n",
        "      # batch, seq_len, num_head,head_dim\n",
        "      q = q.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "      k = k.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "      v = v.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "\n",
        "      # batch, num_head, seq_len,head_dim\n",
        "      q = q.transpose(1,2)\n",
        "      k = k.transpose(1,2)\n",
        "      v = v.transpose(1,2)\n",
        "\n",
        "\n",
        "      # # q:batch, num_head, seq_len,head_dim, k:q:batch, num_head,head_dim, seq_len\n",
        "      # # -->batch, num_head,seq_len,seq_len\n",
        "      att_score = q @ k.transpose(2,3)\n",
        "      att_score.masked_fill_(mask[:seq_len,:seq_len].bool(),-torch.inf)\n",
        "\n",
        "      # # -->att_weight: batch, num_head,seq_len,seq_len, v:batch, num_head, seq_len,head_dim\n",
        "      att_weight = torch.softmax(att_score/k.shape[-1]**0.5,dim = -1)\n",
        "\n",
        "      # -->con_vec: batch, num_head,seq_len,head_dim\n",
        "      con_vec = att_weight @ v\n",
        "\n",
        "      # -->con_vec:batch, seq_len,num_head,head_dim\n",
        "      con_vec = con_vec.transpose(1,2)\n",
        "\n",
        "      con_vec = con_vec.contiguous().view(b,seq_len,self.d_out)\n",
        "\n",
        "      return con_vec\n"
      ],
      "metadata": {
        "id": "P3HE7OIeJFQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Layer_Norm"
      ],
      "metadata": {
        "id": "JP4a4iX8JQW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.ones(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "      mean = x.mean(dim=1,keepdim=True)\n",
        "      var = x.var(dim=1,keepdim=True)\n",
        "      n_x = ((x-mean)/torch.sqrt(var+self.eps))\n",
        "      return self.scale*n_x+self.shift"
      ],
      "metadata": {
        "id": "eRdcY6qeKQa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GELU"
      ],
      "metadata": {
        "id": "RrV9V42VNKo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n"
      ],
      "metadata": {
        "id": "HTp9T-YbNI_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pdZDq84NtWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FF"
      ],
      "metadata": {
        "id": "LyVTzNQtNk9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Linear(emb_dim,4*emb_dim),\n",
        "        GELU(),\n",
        "        nn.Linear(4*emb_dim,emb_dim)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.block(x)\n"
      ],
      "metadata": {
        "id": "L3hh8fEfStMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer Block"
      ],
      "metadata": {
        "id": "08i9RJyqWNL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.a = nn.Sequential(\n",
        "        NormLayer(cfg[\"emb_dim\"]),\n",
        "        MHA(cfg[\"emb_dim\"],cfg[\"emb_dim\"],cfg[\"n_heads\"],cfg[\"context_length\"],cfg[\"drop_rate\"],cfg[\"qkv_bias\"]),\n",
        "        nn.Dropout(cfg['drop_rate']))\n",
        "\n",
        "    self.b = nn.Sequential(\n",
        "        NormLayer(cfg[\"emb_dim\"]),\n",
        "        FeedForward(cfg['emb_dim']),\n",
        "        nn.Dropout(cfg['drop_rate']))\n",
        "\n",
        "  def forward(self,x):\n",
        "    block_a = self.a(x)+x\n",
        "    block_b = self.b(block_a)+block_a\n",
        "    return block_b\n",
        "\n"
      ],
      "metadata": {
        "id": "Xzw1npecWKVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Before Here"
      ],
      "metadata": {
        "id": "pPkzMBfoNM4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "batch = torch.stack((inputs,inputs))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class TransformerBlock(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         return x\n",
        "\n",
        "# class GPTModel(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "\n",
        "#     def forward(self, in_idx):\n",
        "#         return\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1DSyMZvwYIb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "batch = []\n",
        "txt1 = 'Every effort moves you'\n",
        "txt2 = 'Every day holds a'\n",
        "batch.append(torch.tensor(tk.encode(txt1)))\n",
        "batch.append(torch.tensor(tk.encode(txt2)))\n",
        "batch = torch.stack(batch,dim=0)\n",
        "\n",
        "out = model(batch)\n",
        "# print(\"Input batch:\\n\", batch)\n",
        "# print(\"\\nOutput shape:\", out.shape)\n",
        "out.shape\n",
        "\n",
        "params = sum([p.numel() for p in model.parameters()])\n",
        "params"
      ],
      "metadata": {
        "id": "XDnt5rTboAPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "31a9f363-5fa6-4bd2-ee11-12be7615b0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mask' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2582593237.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# print(\"Input batch:\\n\", batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# print(\"\\nOutput shape:\", out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2713559513.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_embeds\u001b[0m  \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrf_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2210840573.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mblock_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mblock_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mblock_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2542341639.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m# # -->batch, num_head,seq_len,seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0matt_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0matt_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;31m# # -->att_weight: batch, num_head,seq_len,seq_len, v:batch, num_head, seq_len,head_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1n2zGDtYIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text 0724"
      ],
      "metadata": {
        "id": "oRmoUETkkH0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # run above code ,load the standard GPT model\n",
        "# model = GPTModel(GPT_CONFIG_124M)\n",
        "# def generate_text_sample(model,idx, max_new_tokens,context_size ):\n",
        "#   a = []\n",
        "#   for _ in range(max_new_tokens):\n",
        "#     idx_cond = idx[:,-context_size:]\n",
        "#     with torch.no_grad():\n",
        "#       logits=model(idx_cond)\n",
        "#     logits = logits[:,-1,:]\n",
        "#     probas = torch.softmax(logits,dim=-1)\n",
        "#     id_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
        "#     idx = torch.cat((idx,id_next),dim=-1)\n",
        "#   # return logits\n",
        "#   # return probas\n",
        "#   return idx\n",
        "\n",
        "# # def generate_text_sample(idx, max_new_tokens=6,context_size=5 ):\n",
        "# #   print(idx[:,-context_size:])\n",
        "\n",
        "# start_text = 'Hello, I am '\n",
        "# encoded = tk.encode(start_text)\n",
        "# encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "# model.eval()\n",
        "# out = generate_text_sample(model,encoded_tensor,max_new_tokens=10,context_size=CFG['context_length'])\n",
        "# print(out)\n",
        "# print(tk.decode(out.squeeze(0).tolist()))\n",
        "# # generate_text_sample(model,encoded_tensor,max_new_tokens=6,context_size=3)\n",
        "\n",
        "# # len(generate_text_sample(model,encoded_tensor,max_new_tokens=6,context_size=3)\n",
        "\n",
        "# start_text = 'Hello, I am '\n",
        "# encoded = tk.encode(start_text)\n",
        "# encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "# model.eval()\n",
        "# out = generate_text_sample(model,encoded_tensor,max_new_tokens=10,context_size=CFG['context_length'])\n",
        "# print(out)\n",
        "# print(tk.decode(out.squeeze(0).tolist()))\n",
        "# # generate_text_sample(model,encoded_tensor,max_new_tokens=6,context_size=3)\n",
        "\n",
        "# # len(generate_text_sample(model,encoded_tensor,max_new_tokens=6,context_size=3)"
      ],
      "metadata": {
        "id": "DhT-JfllkLh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###small tips about cat and stack"
      ],
      "metadata": {
        "id": "pok0dHxYuHQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(([1,2,3],[4,5,6],[7,8,9],[10,11,12]))\n",
        "b = torch.tensor(([81,82,83],[84,85,86],[87,88,89],[90,91,92]))\n",
        "a.shape,b.shape\n",
        "torch.stack((a,b),dim=0),torch.stack((a,b),dim=0).shape,\n",
        "# torch.stack((a,b),dim=0).shape,  torch.cat((a,b),dim=0).shape\n",
        "# torch.stack((a,b),dim=2)\n",
        "# torch.cat((a,b),dim=2)"
      ],
      "metadata": {
        "id": "7-PFyfhmuL_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###here0724"
      ],
      "metadata": {
        "id": "FxpbcLCKviGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start_text = 'Hello, I am '\n",
        "# encoded = tk.encode(start_text)\n",
        "# encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "# model.eval()\n",
        "# out = generate_text_sample(model,encoded_tensor,max_new_tokens=10,context_size=CFG['context_length'])\n",
        "# print(out)\n",
        "# print(tk.decode(out.squeeze(0).tolist()))\n",
        "# generate_text_sample(model,encoded_tensor,max_new_tokens=6,context_size=3)\n",
        "\n",
        "# len(generate_text_sample(model,encoded_tensor,max_new_tokens=6,context_size=3))\n"
      ],
      "metadata": {
        "id": "rIb0MLpwYIZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###test0724"
      ],
      "metadata": {
        "id": "xK2xeu1-4QIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a =nn.Embedding(10000,3)\n",
        "b = nn.Embedding(4,3)\n",
        "a(batch), b(torch.arange(4))\n",
        "\n",
        "# batch"
      ],
      "metadata": {
        "id": "LJQUq2tckGvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################Play Ground######################################################Play Ground#################################\n",
        "#####################Play Ground######################################################Play Ground#################################\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        "\n",
        ")\n",
        "batch = torch.stack((inputs,inputs))\n",
        "testa = inputs[0].unsqueeze(0)\n",
        "testb = torch.stack((inputs[0],inputs[1]))\n",
        "\n",
        "# a = LayerNorm(3)\n",
        "# a = GELU()\n",
        "# a(testb),testb\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "# a = MHA(3,10,dropout=0,context_length=15,num_heads=2)\n",
        "# batch.shape,a(batch).shape\n",
        "\n",
        "# for name, param in a.named_parameters():\n",
        "#     print(name, param.shape)\n",
        "# for name, buf in a.named_buffers():\n",
        "#     print(name, buf.shape)\n",
        "\n",
        "# a = nn.Linear(3,4,bias=False)\n",
        "# for c,d in a.named_parameters():\n",
        "#   print('Linearw',d.shape)\n",
        "\n",
        "# print('b',testb.shape),\n",
        "# print('a(b)',a(testb).shape)\n",
        "# a = FeedForward(3)\n",
        "# a(testb),a(testb).shape\n",
        "# a = NormLayer(3)\n",
        "# a = MHA(3,2,dropout=0,context_length=15,num_heads=2)\n",
        "# a(batch).shape\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "# GPT_test = torch.rand(2,4,768)\n",
        "# TB = TransformerBlock(gd)\n",
        "# TB(GPT_test).shape\n",
        "\n",
        "# a = torch.tensor([[0, 0, 0,  0],\n",
        "#                   [0, 0, 0,  0],\n",
        "#                   [0, 0, 0,  0]])\n",
        "# t = torch.stack((a,a))\n",
        "# b = torch.tensor([[1, 1, 1,  1],\n",
        "#                   [1, 1, 1,  1],\n",
        "#                   [1, 1, 1,  1]])\n",
        "\n",
        "# t+b\n",
        "\n",
        "\n",
        "gd = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "tk = tiktoken.get_encoding('gpt2')\n",
        "test_txt =  ['Every effort moves you' ,'Every day holds a']\n",
        "GPT_input = torch.tensor([tk.encode(i) for i in test_txt])\n",
        "# GPT_input\n",
        "\n",
        "a = GPTforward(gd)\n",
        "a(GPT_input).shape\n",
        "\n",
        "pnum = sum(p.numel() for p in a.parameters())\n",
        "pnum"
      ],
      "metadata": {
        "id": "G0pL6OqRof_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-Af63iSrEpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chapter5 Recap 0801"
      ],
      "metadata": {
        "id": "3cHBKpS7rHHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        "\n",
        ")\n",
        "batch = torch.stack((inputs,inputs))"
      ],
      "metadata": {
        "id": "OivSRmwcsCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import tiktoken\n",
        "tk = tiktoken.get_encoding('gpt2')\n",
        "gd = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "IVE6zrnSrKaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MHA0801"
      ],
      "metadata": {
        "id": "x9wqjoxq4vV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "  def __init__(self,d_in,d_out,qkv_bias,num_head,context_len,drop):\n",
        "    super().__init__()\n",
        "    self.num_head = num_head\n",
        "    self.d_out = d_out\n",
        "    self.head_dim = d_out // num_head\n",
        "    self.dropout = nn.Dropout(drop)\n",
        "    assert (d_out % num_head == 0), \"d_out must be divisible by num_heads\"\n",
        "    self.wq = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.wk = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.wv = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_len,context_len),1))\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_token,d_in = x.shape\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "    q = q.view(b,num_token,self.num_head,self.head_dim) #b,context_len,num_head,head_dim\n",
        "    k = k.view(b,num_token,self.num_head,self.head_dim) #b,context_len,num_head,head_dim\n",
        "    v = v.view(b,num_token,self.num_head,self.head_dim) #b,context_len,num_head,head_dim\n",
        "\n",
        "    q = q.transpose(1,2) #b,num_head,context_len,head_dim\n",
        "    k = k.transpose(1,2) #b,num_head,context_len,head_dim\n",
        "    v = v.transpose(1,2) #b,num_head,context_len,head_dim\n",
        "\n",
        "    att_score = q @ k.transpose(2,3)\n",
        "    mask_bool = self.mask.bool()[:num_token, :num_token]\n",
        "    att_score = att_score.masked_fill(mask_bool,-torch.inf)\n",
        "    att_weight = torch.softmax(att_score,dim=-1)\n",
        "    con_vec = att_weight @ v\n",
        "    con_vec = self.dropout(con_vec)\n",
        "    con_vec = con_vec.transpose(1,2)\n",
        "    con_vec = con_vec.contiguous().view(b,num_token,self.d_out)\n",
        "    con_vec = self.out_proj(con_vec)\n",
        "\n",
        "\n",
        "    return con_vec\n",
        "\n",
        "a = MHA(3,10,False,2,context_len=100,drop=0.5)\n",
        "a(batch).shape"
      ],
      "metadata": {
        "id": "NViOi36k4u6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NormLayer0801"
      ],
      "metadata": {
        "id": "-Y0Sh2ot416X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormLayer(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.eps = 1e-5\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = torch.mean(x,dim=-1,keepdim=True)\n",
        "    var = torch.var(x,dim=-1,keepdim=True)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        "\n",
        ")\n",
        "batch = torch.stack((inputs,inputs))\n",
        "a = NormLayer(3)\n",
        "a(inputs)[0].var()"
      ],
      "metadata": {
        "id": "EIqQu8Ls45KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GELU"
      ],
      "metadata": {
        "id": "STG9sfYstNqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "NRBBRXK2tRoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FF"
      ],
      "metadata": {
        "id": "oQ786vmgtfZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "Bq3SkkgdthH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NS"
      ],
      "metadata": {
        "id": "xjLYfk7l45if"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 5**"
      ],
      "metadata": {
        "id": "V2QgWJW5KRzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "import tiktoken\n",
        "tk = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "k4Oe6aJFOlye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gptModel"
      ],
      "metadata": {
        "id": "yy3LC1T8KmvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gd = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class NormLayer(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = NormLayer(cfg[\"emb_dim\"])\n",
        "        self.norm2 = NormLayer(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = NormLayer(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(gd)\n",
        "\n",
        "batch = []\n",
        "txt1 = 'Every effort moves you'\n",
        "txt2 = 'Every day holds a'\n",
        "batch.append(torch.tensor(tk.encode(txt1)))\n",
        "batch.append(torch.tensor(tk.encode(txt2)))\n",
        "batch = torch.stack(batch,dim=0)\n",
        "(batch.shape)\n",
        "out = model(batch)\n",
        "# print(\"Input batch:\\n\", batch)\n",
        "# print(\"\\nOutput shape:\", out.shape)\n",
        "# out.shape\n",
        "\n",
        "# params = sum([p.numel() for p in model.parameters()])\n",
        "# params"
      ],
      "metadata": {
        "id": "pqZpFB3JKWOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "id": "RkS0fvL1Gzv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b4bf43-7d41-47ef-9fd0-1d3787d1cba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##shortcut0731"
      ],
      "metadata": {
        "id": "PVn3Nz4l7vNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, idx,max_new,context_size):\n",
        "  a = []\n",
        "  for _ in range(max_new):\n",
        "    id_cond = idx[:,-context_size:]\n",
        "    out = model(id_cond)\n",
        "    last = out[:,-1,:]\n",
        "    probas = torch.softmax(last,dim=-1)\n",
        "    id_next = torch.argmax(probas,keepdim=True)\n",
        "    idx = torch.cat((idx, id_next), dim=-1)\n",
        "  return idx\n",
        "\n",
        "\n",
        "\n",
        "def text2token(text):\n",
        "  tik = tk\n",
        "  encode = tik.encode(text,allowed_special={'<|endoftext|>'})\n",
        "  encode = torch.tensor(encode)\n",
        "  encode= encode.unsqueeze(0)\n",
        "  return encode\n",
        "\n",
        "def token2text(token):\n",
        "  tik = tk\n",
        "  token=token.squeeze(0)\n",
        "  token = token.tolist()\n",
        "  decode = tik.decode(token)\n",
        "  return decode\n",
        "a = 'Every effort moves you'\n",
        "b = text2token(a)\n",
        "c = token2text(b)\n",
        "c"
      ],
      "metadata": {
        "id": "-gLQqimr7uiq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1a7de86f-27e6-4387-defc-c1e7df2a93a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every effort moves you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gd = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(gd)\n",
        "# model.eval()\n",
        "\n"
      ],
      "metadata": {
        "id": "tLIw5hphxczI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tids = generate_text(model,text2token('Every effort moves you'),10,gd['context_length'])\n",
        "token2text(tids)"
      ],
      "metadata": {
        "id": "65rvL3xMxuSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1420569b-47a5-4c2f-d42f-c192cb9d3d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every effort moves you Samoa parad Defensive MacBook Referospace preparation Einstein ShepherdMot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#manual CE\n",
        "inputs1 = text2token('every effort moves')\n",
        "inputs2 = text2token('I really like')\n",
        "target1 = text2token(' effort moves you')\n",
        "target2 = text2token(' really like chocolate')\n",
        "\n",
        "inputs = torch.cat((inputs1,inputs2),dim=0)\n",
        "targets = torch.cat((target1,target2),dim=0)\n",
        "\n",
        "inputs,targets\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = model(inputs)\n",
        "probas = torch.softmax(logits,dim=-1)\n",
        "probas.shape,logits.shape\n",
        "# # probas.shape\n",
        "target_probas_1 = probas[0,[0,1,2],targets[0]]\n",
        "target_probas_2 = probas[1,[0,1,2],targets[1]]\n",
        "\n",
        "print(target_probas_1)\n",
        "print(target_probas_2)\n",
        "log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
        "neg_avg = -log_probas.mean()\n",
        "neg_avg\n",
        "# log_probas\n"
      ],
      "metadata": {
        "id": "_mYFgNrw0ueo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5828780-e187-4f48-9bb0-57c82771ca48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.0363e-05, 2.7361e-05, 1.1445e-05])\n",
            "tensor([8.2989e-06, 9.1948e-05, 6.1120e-06])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.8335)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(123)\n",
        "# a = torch.randn((2,3,5))\n",
        "# a\n",
        "# a.flatten(0,1)\n",
        "# a.contiguous().view(6,5)"
      ],
      "metadata": {
        "id": "cxUX7ZuRdw5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#auto CE\n",
        "logits.shape,targets.shape\n",
        "l_flat = logits.flatten(0,1)\n",
        "# l_flat = logits.contiguous().view(6,50257)\n",
        "t_flat = targets.flatten(0,1)\n",
        "# t_flat = targets.contiguous().view(6)\n",
        "l_flat.shape,t_flat.shape\n",
        "ce = nn.CrossEntropyLoss()\n",
        "loss = ce(l_flat,t_flat)\n",
        "# loss = nn.functional.cross_entropy(l_flat,t_flat)\n",
        "loss\n",
        "torch.exp(loss)"
      ],
      "metadata": {
        "id": "WBDm9Ev0Xem5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254aa155-6461-4981-9a1a-9136bcf7dba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(50689.4570)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "se_87dKy0tuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##text generator"
      ],
      "metadata": {
        "id": "MVqaCnjSKpPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_sample(model,idx, max_new_tokens,context_size ):\n",
        "  a = []\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:,-context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits=model(idx_cond)\n",
        "    logits = logits[:,-1,:]\n",
        "    probas = torch.softmax(logits,dim=-1)\n",
        "    id_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
        "    idx = torch.cat((idx,id_next),dim=-1)\n",
        "  return idx\n",
        "\n",
        "def t2to(txt,tok=tk):\n",
        "  encoded=tok.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def to2t(tid,tok=tk):\n",
        "  tid = tid.squeeze(0)\n",
        "  decoded=tok.decode(tid.tolist())\n",
        "  return decoded\n",
        "\n"
      ],
      "metadata": {
        "id": "vZB3nlahL24j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gd = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(gd)\n",
        "model.eval()\n",
        "tids = generate_text_sample(model,t2to('Every effort moves you'),10,gd['context_length'])\n",
        "to2t(tids)"
      ],
      "metadata": {
        "id": "Bgl4VliHMLqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0afa7ccf-fdd6-4555-8746-72bd6c6c9296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every effort moves you rentingetic wasnم refres RexMeCHicular stren'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False)"
      ],
      "metadata": {
        "id": "14zrZJa5PNWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CE"
      ],
      "metadata": {
        "id": "RLH9YhGuXfNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#manual CE\n",
        "inputs1 = t2to('every effort moves')\n",
        "inputs2 = t2to('I really like')\n",
        "t1 = t2to(' effort moves you')\n",
        "t2 = t2to(' really like chocolate')\n",
        "\n",
        "inputs = torch.cat((inputs1,inputs2),dim=0)\n",
        "targets = torch.cat((t1,t2),dim=0)\n",
        "\n",
        "inputs.shape\n",
        "\n",
        "# targets\n",
        "# with torch.no_grad():\n",
        "#   logits = model(inputs)\n",
        "# probas = torch.softmax(logits,dim=-1)\n",
        "\n",
        "# # probas.shape\n",
        "# target_probas_1 = probas[0,[0,1,2],targets[0]]\n",
        "# target_probas_2 = probas[1,[0,1,2],targets[1]]\n",
        "\n",
        "# # print(target_probas_1)\n",
        "# # print(target_probas_2)\n",
        "# log_probas = torch.log(torch.cat((target_probas_1,target_probas_2)))\n",
        "# neg_avg = -log_probas.mean()\n",
        "# neg_avg"
      ],
      "metadata": {
        "id": "RT9A-b5ZD1yZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95300cc6-c778-4d88-a5e6-77bd4ba5ab02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#auto CE\n",
        "logits.shape,targets.shape\n",
        "l_flat = logits.flatten(0,1)\n",
        "t_flat = targets.flatten(0,1)\n",
        "# loss = nn.cross_entropy(l_flat,t_flat)\n",
        "# ce = nn.CrossEntropyLoss()\n",
        "# loss = ce(l_flat,t_flat)\n",
        "loss = nn.functional.cross_entropy(l_flat,t_flat)\n",
        "loss\n",
        "# torch.exp(loss)"
      ],
      "metadata": {
        "id": "-0SGnsLvXi2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87bdbb43-fbd7-4b83-c630-df55e8d22946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.8335)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##dataloader"
      ],
      "metadata": {
        "id": "I5DjniD9tAYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a = torch.arange(11)\n",
        "# ws = 3\n",
        "# stride = 2\n",
        "# for i in range(0,len(a)-ws,stride):\n",
        "#   print(a[i:i+ws])\n",
        "#   print(a[i+1:i+ws+1])\n",
        "#   print()\n",
        "# len(a)-ws"
      ],
      "metadata": {
        "id": "YtJEiAqilYjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "class DatasetGPT(Dataset):\n",
        "  def __init__(self,txt,tokenizer,window_size,stride):\n",
        "    super().__init__()\n",
        "    self.input_idx = []\n",
        "    self.target_idx = []\n",
        "    token = tokenizer.encode(txt)\n",
        "    for i in range(0, len(token)-window_size,stride):\n",
        "      input_chunk=token[i:i+window_size]\n",
        "      target_chunk=token[i+1:i+window_size+1]\n",
        "      self.input_idx.append(torch.tensor(input_chunk))\n",
        "      self.target_idx.append(torch.tensor(target_chunk))\n",
        "\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    return self.input_idx[i],self.target_idx[i]\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.input_idx)\n",
        "\n",
        "\n",
        "def dataloaderV2(txt,batch_size,window_size,stride,shuffle=True,drop_last=True,num_workers=0):\n",
        "  dataset = DatasetGPT(txt,tk,window_size,stride)\n",
        "  dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ag9nGQ27KNFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data"
      ],
      "metadata": {
        "id": "yGdnekbzUhfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('the-verdict.txt','r',encoding='utf-8') as f:\n",
        "  raw = f.read()\n",
        "raw\n",
        "\n",
        "split_idx = int(0.9*len(raw))\n",
        "train_data = raw[:split_idx]\n",
        "valid_data = raw[split_idx:]\n",
        "\n",
        "train_loader = dataloaderV2(train_data,batch_size=2,window_size=gd['context_length'],stride=gd['context_length'],drop_last=True,shuffle=True,num_workers=0)\n",
        "valid_loader = dataloaderV2(valid_data,batch_size=2,window_size=gd['context_length'],stride=gd['context_length'],drop_last=False,shuffle=False,num_workers=0)\n",
        "# x,y = next(iter(train_loader))\n",
        "# token2text(x[0]),token2text(y[0])"
      ],
      "metadata": {
        "id": "UKSf07ifUg0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in train_loader:\n",
        "  print(x.shape,y.shape)\n",
        "print()\n",
        "for x,y in valid_loader:\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "len(train_loader)"
      ],
      "metadata": {
        "id": "M1x5oN9EWduE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b250f1df-7cd9-4843-cb30-09455714f4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def closs(in_batch,tar_batch,model,device):\n",
        "  in_batch = in_batch.to(device)\n",
        "  tar_batch = tar_batch.to(device)\n",
        "  model = model.to(device)\n",
        "  logit = model(in_batch)\n",
        "  # logit = logit.flatten(0,1)\n",
        "  # tar = tar_batch.flatten()\n",
        "  loss = torch.nn.functional.cross_entropy(logit.flatten(0,1),tar_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "def closs_loader(dataload,model,device,num_batch):\n",
        "  total_loss = 0\n",
        "  model = model.to(device)\n",
        "  if len(dataload)==0:\n",
        "    return float('nan')\n",
        "  elif num_batch==None:\n",
        "    num_batch=len(dataload)\n",
        "  else:\n",
        "    num_batch = min(num_batch,len(dataload))\n",
        "  for i,(x,y) in enumerate(dataload):\n",
        "    if i<num_batch:\n",
        "      loss = closs(x,y,model,device)\n",
        "      total_loss+=loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss/num_batch\n",
        "x,y = next(iter(train_loader))\n",
        "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "torch.manual_seed(123)\n",
        "model=GPTModel(gd)\n",
        "# model(x).shape, y.shape\n",
        "closs(x,y,model,device)\n",
        "x.numel(),y.numel()\n",
        "# with torch.no_grad():\n",
        "#   tl = closs_loader(train_loader,model,device,num_batch=None)\n",
        "#   vl = closs_loader(valid_loader,model,device,num_batch=None)\n",
        "# print(tl,vl)"
      ],
      "metadata": {
        "id": "SatQo9_csotL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e60a360-bd2c-4640-b4e6-4bfbaf6a70c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(input_batch,target_batch,model,device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "def calc_loss_loader(data_loader,model,device,num_batch=None):\n",
        "  total_loss=0\n",
        "  if len(data_loader) == 0:\n",
        "    return float('nan')\n",
        "  elif num_batch is None:\n",
        "    num_batch = len(data_loader)\n",
        "  else:\n",
        "    num_batch = min(num_batch,len(data_loader))\n",
        "  for i,(ib,tb) in enumerate(data_loader):\n",
        "    if i< num_batch:\n",
        "      loss = calc_loss(ib,tb,model=model,device=device)\n",
        "      total_loss+=loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss/num_batch\n",
        "\n",
        "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "torch.manual_seed(123)\n",
        "model=GPTModel(gd)\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader,model,device)\n",
        "  valid_loss = calc_loss_loader(valid_loader,model,device)\n",
        "\n",
        "print(train_loss)\n",
        "print(valid_loss)\n"
      ],
      "metadata": {
        "id": "R0d_A2EpjXM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "375bab4b-2d0d-4086-bd16-4b87b748dc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.987917476230198\n",
            "10.976283073425293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z97cTsVcPTvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##playgroundCP5"
      ],
      "metadata": {
        "id": "tiEYQNlYP1wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = t2to('every effort moves')\n",
        "inputs2 = t2to('I really like')\n",
        "inputs = torch.cat((inputs1,inputs2),dim=0)\n",
        "inputs\n",
        "# inputs4 = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "#                        [40,    1107, 588]])   #  \"I really like\"]\n",
        "\n",
        "t1 = t2to(' effort moves you')\n",
        "t2 = t2to(' really like chocolate')\n",
        "# t1,t2\n",
        "targets = torch.cat((t1,t2),dim=0)"
      ],
      "metadata": {
        "id": "q17Wl_ztP6YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDFCFCbmGcb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-tQXuuLGh75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 5 (rebuild)**"
      ],
      "metadata": {
        "id": "qJMRDg7TzPCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"matplotlib\",\n",
        "        \"numpy\",\n",
        "        \"tiktoken\",\n",
        "        \"torch\",\n",
        "        \"tensorflow\" # For OpenAI's pretrained weights\n",
        "       ]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yebHMtEOzSPS",
        "outputId": "7bb588c0-2419-419f-aaaf-5217922d6bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matplotlib version: 3.10.0\n",
            "numpy version: 2.0.2\n",
            "tiktoken version: 0.11.0\n",
            "torch version: 2.8.0+cu126\n",
            "tensorflow version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 ############write by own\n",
        "import torch\n",
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size':50257,\n",
        "    'context_length':256,\n",
        "    'emb_dim':768,\n",
        "    'n_heads':12,\n",
        "    'n_layers':12,\n",
        "    'drop_rate':0.1,\n",
        "    'qkv_bias':False\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model=GPTModel(GPT_CONFIG_124M)\n",
        "model = model.to('cpu')\n",
        "model.eval()\n",
        "#############write by own"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMOJxfmk7hKr",
        "outputId": "d9b5e9a4-d64a-463c-a19f-569333165fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3972379531.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1 ############write by own\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m GPT_CONFIG_124M = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50257\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'context_length'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 ############write by own\n",
        "import tiktoken\n",
        "def text2token(text,tokenizer):\n",
        "  encoded = tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "def token2text(token,tokenizer):\n",
        "  flat = token.squeeze(0)\n",
        "  text = tokenizer.decode(flat.tolist())\n",
        "  return text\n",
        "\n",
        "def generate_t_sample(model,idx,context_size,max_generate):\n",
        "  for _ in range(max_generate):\n",
        "    id_calc = idx[:,-context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(id_calc)\n",
        "      probas = torch.softmax(logits,dim=-1)\n",
        "      focus = probas[:,-1,:]\n",
        "      id_next = torch.argmax(focus,keepdim=True)\n",
        "      idx = torch.cat((idx,id_next),dim=-1)\n",
        "  return idx\n",
        "\n",
        "\n",
        "start_text = 'Every effort moves you'\n",
        "tk = tiktoken.get_encoding('gpt2')\n",
        "tokens = generate_t_sample(model,text2token(start_text,tk),GPT_CONFIG_124M['context_length'],10)\n",
        "token2text(tokens,tk)\n",
        "#############write by own\n",
        "# a = text2token(start_text,tk)\n",
        "\n",
        "# out = model(a)\n",
        "# probas = torch.softmax(out,dim=-1)\n",
        "# torch.argmax(probas[:,-1,:],keepdim=True).shape"
      ],
      "metadata": {
        "id": "zIwWLt327nWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 ############write by own\n",
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
        "#############write by own\n",
        "# token2text(inputs[0],tk)"
      ],
      "metadata": {
        "id": "zaaihZhi7oNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 ############write by own\n",
        "with torch.no_grad():\n",
        "  logits = model(inputs)\n",
        "probas = torch.softmax(logits, dim=-1)\n",
        "print(probas.shape)\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "jpLl1IQI7o6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 ############write by own\n",
        "token_ids = torch.argmax(probas,dim=-1,keepdim=True)\n",
        "print(token_ids)\n",
        "#############write by own\n",
        "token2text(token_ids[0].flatten(),tk)\n",
        "# token_ids[0].flatten()"
      ],
      "metadata": {
        "id": "JG-u_1tP7plu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 ############write by own\n",
        "print(token2text(targets[0],tk))\n",
        "print(token2text(token_ids[0].flatten(),tk))\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "hDCg5PXb7qNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 ############write by own\n",
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx,[0,1,2],targets[text_idx]]\n",
        "\n",
        "target_probas_1.shape\n",
        "target_probas_2.shape\n",
        "target_probas_1,target_probas_2\n",
        "#############write by own\n",
        "# targets[0].shape\n",
        "target_probas_1.shape,target_probas_2.shape"
      ],
      "metadata": {
        "id": "UNh3HRTN7qy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 ############write by own\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "fq0WPW527rWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9  ############write by own\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "avg_log_probas\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "mcrziJCf7sJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 ############write by own\n",
        "# cross enotropy(for one-hot):H(P,Q)=−logq\n",
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "mc0K5NIM7stC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11 ############write by own\n",
        "print(logits.shape)\n",
        "print(targets.shape)\n",
        "\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "dKvzBdZm7tSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 ############write by own\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "#############write by own\n",
        "logits_flat,targets_flat"
      ],
      "metadata": {
        "id": "SlKoJbzK7t0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 13 ###########write by own\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat,targets_flat)\n",
        "loss\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "0OlBDX9s7ue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 ############write by own\n",
        "preplexity = torch.exp(loss)\n",
        "preplexity\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "vtcTRaMj7u9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 ############write by own\n",
        "with open('the-verdict.txt', \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "rjF7zrT_7vn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 ############write by own\n",
        "print(text_data[:99])\n",
        "print(text_data[-99:])\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "Dwm6i1eN7wKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17 ############write by own\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tk.encode(text_data))\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "pW61RgS67wzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18 ############write by own\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "class DatasetGPT(Dataset):\n",
        "  def __init__(self,txt,window_size,stride,tokenizer=tk):\n",
        "    super().__init__()\n",
        "    enc = tk.encode(txt)\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    for i in range(0,len(enc)-window_size,stride):\n",
        "      xx = enc[i:i+window_size]\n",
        "      yy = enc[i+1:i+window_size+1]\n",
        "      self.x.append(torch.tensor(xx))\n",
        "      self.y.append(torch.tensor(yy))\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index],self.y[index]\n",
        "\n",
        "def dataloaderV2(txt,batch_size,window_size,stride,shuffle=True,drop_last=True,num_workers=0):\n",
        "  dataset = DatasetGPT(txt=txt,window_size=window_size,stride=stride)\n",
        "  dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
        "  return dataloader\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "KiWOd50-7ySX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 ############write by own\n",
        "train_ratio=0.9\n",
        "split_idx = int(train_ratio*len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "valid_data = text_data[split_idx:]\n",
        "train_loader = dataloaderV2(train_data,batch_size=2,window_size=GPT_CONFIG_124M['context_length'],stride=GPT_CONFIG_124M['context_length'],drop_last=True,shuffle=True,num_workers=0)\n",
        "valid_loader = dataloaderV2(valid_data,batch_size=2,window_size=GPT_CONFIG_124M['context_length'],stride=GPT_CONFIG_124M['context_length'],drop_last=False,shuffle=False,num_workers=0)\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "aF-mHtqFApn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 ############write by own\n",
        "for x,y in train_loader:\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "print()\n",
        "for x,y in valid_loader:\n",
        "  print(x.shape,y.shape)\n",
        "len(train_loader)\n",
        "#############write by own\n",
        "# for i,(x,y) in enumerate(train_loader):\n",
        "#   if i==1:\n",
        "#     break\n",
        "#   xxx = x\n",
        "#   yyy = y\n",
        "  # print(x.shape,y.shape)\n",
        "  # print(x,y)\n",
        "# xxx[0],yyy[0] # matched"
      ],
      "metadata": {
        "id": "HoVCf-d77z7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 ############write by own\n",
        "train_tokens=0\n",
        "for x,y in train_loader:\n",
        "  train_tokens+=x.numel()\n",
        "\n",
        "valid_tokens=0\n",
        "for x,y in valid_loader:\n",
        "  valid_tokens+=x.numel()\n",
        "train_tokens,valid_tokens\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "ykKTBtXy70oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 ############write by own\n",
        "def cal_loss_batch(inp,tar,model,device):\n",
        "  inp = inp.to(device)\n",
        "  tar = tar.to(device)\n",
        "  logits = model(inp)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),tar.flatten())\n",
        "  return loss\n",
        "\n",
        "def cal_loss_loader(dataloader,model,device,num_batch=None):\n",
        "  total_loss = 0\n",
        "  if num_batch==0:\n",
        "    return float('nan')\n",
        "  elif num_batch == None:\n",
        "    num_batch = len(dataloader)\n",
        "  else:\n",
        "    num_batch = min(len(dataloader),num_batch)\n",
        "  for i, (inp,tar) in enumerate(dataloader):\n",
        "    if i>= num_batch:\n",
        "      break\n",
        "    else:\n",
        "      total_loss+=cal_loss_batch(inp,tar,model,device).item()\n",
        "  return total_loss/num_batch\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "Id7KE1SG71OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 ############write by own\n",
        "# cal_loss_loader(dataloader,model,device,num_batch=None):\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = cal_loss_loader(train_loader,model,device)\n",
        "  valid_loss = cal_loss_loader(valid_loader,model,device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", valid_loss)\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "i96NT2R27129"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tk = tiktoken.get_encoding('gpt2')\n",
        "# context_size = model.pos_emb.weight.shape[0]\n",
        "# start_text = 'hello'\n",
        "# start_text = text2token(start_text,tk).to(device)\n",
        "# start_text\n",
        "# a = generate_text(model,idx=start_text,max_new=50,context_size=context_size)\n",
        "# a\n",
        "# token2text(a,tk)"
      ],
      "metadata": {
        "id": "0fpgA8tfIB4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_sample(model,device,start_text,tokenizer):\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  start_text = text2token(start_text,tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    # (model,idx=start_text,max_new=50,context_size=context_size)\n",
        "    gt = generate_t_sample(model,idx=start_text,context_size=context_size,max_generate=50)\n",
        "    decoded_text = token2text(gt, tokenizer)\n",
        "    return decoded_text\n",
        "gen_sample(model,device,start_text='hello',tokenizer=tk)"
      ],
      "metadata": {
        "id": "spd2XBFQHJCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 ############write by own\n",
        "def gen_sample(model,device,start_text,tokenizer,max_generate=50):\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  start_text = text2token(start_text,tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    gt = generate_t_sample(model,idx=start_text,context_size=context_size,max_generate=max_generate)\n",
        "    decoded_text = token2text(gt, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))\n",
        "  model.train()\n",
        "\n",
        "def eval_model(model,train_loader,valid_loader,device,eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = cal_loss_loader(train_loader,model,device,num_batch=eval_iter)\n",
        "    valid_loss = cal_loss_loader(valid_loader,model,device,num_batch=eval_iter)\n",
        "  model.train()\n",
        "  return train_loss,valid_loss\n",
        "\n",
        "def train_model(train_loader,valid_loader,model,num_epochs,optimizer,device,eval_freq,eval_iter,tokenizer,start_context):\n",
        "  train_losses,valid_losses,track_token_seen = [],[],[]\n",
        "  tokens_seen, global_step = 0, -1\n",
        "  model.train()\n",
        "  for e in range(num_epochs):\n",
        "    for inp,tar in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = cal_loss_batch(inp,tar,model,device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen += inp.numel()\n",
        "      global_step+=1\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss,valid_loss = eval_model(model,train_loader,valid_loader,device,eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        valid_losses.append(valid_loss)\n",
        "        track_token_seen.append(tokens_seen)\n",
        "        print(f'Ep:{e+1} (Step {global_step}) Train loss:{train_loss} valid loss:{valid_loss}')\n",
        "        gen_sample(model,device,start_context,tokenizer)\n",
        "  return train_losses, valid_losses, track_token_seen\n",
        "\n",
        "\n",
        "#############write by own\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "ef = len(train_loader)\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "num_epochs=10\n",
        "train_losses, valid_losses, track_token_seen = train_model(train_loader,valid_loader,model,num_epochs,optimizer,device,eval_freq=ef,eval_iter=5,tokenizer=tokenizer,start_context='Every effort moves')\n"
      ],
      "metadata": {
        "id": "7a3WgCHH72eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNxb2Zven5jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    # ax1.legend(loc=\"upper right\")\n",
        "    # ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # # Create a second x-axis for tokens seen\n",
        "    # ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    # ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    # ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    # fig.tight_layout()  # Adjust layout to make room\n",
        "    # plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, track_token_seen, train_losses, valid_losses)"
      ],
      "metadata": {
        "id": "1x6r0_s75EJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 ############write by own\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "def plot_loss1(ep,tl,vl,ts):\n",
        "  fig,ax1 = plt.subplots(figsize=(5,3))\n",
        "  ax1.plot(ep,tl,label='tl')\n",
        "  ax1.plot(ep,vl,linestyle='--',label='vl')\n",
        "  ax1.set_xlabel(\"Epochs\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(ts, tl, alpha=0)\n",
        "  ax2.set_xlabel(\"token seen\")\n",
        "\n",
        "\n",
        "#############write by own\n",
        "# num_epochs,len(train_losses),len(valid_losses),len(track_token_seen)\n",
        "ep = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_loss1(ep,train_losses,valid_losses,track_token_seen)"
      ],
      "metadata": {
        "id": "FGQPu5ep732l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "# (model=model,idx=text2token(\"Every effort moves you\", tokenizer),max_new_tokens=25,context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "token_ids = generate_t_sample(model,idx=text2token('Every effort moves you',tokenizer),context_size=GPT_CONFIG_124M['context_length'],max_generate=20)\n",
        "print(\"Output text:\\n\", token2text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "Td9LAQ1T69fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")"
      ],
      "metadata": {
        "id": "-2TtRFavCXYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 tempurture scale ############write by own\n",
        "v = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "iv = {v:k for k,v in v.items()}\n",
        "\n",
        "next_tokl = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
        "next_tokl.shape\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "TcSV_UDK74i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24 tempurture scale ############write by own\n",
        "probas = torch.softmax(next_tokl,dim=-1)\n",
        "def count_sample(logits):\n",
        "  probas = torch.softmax(logits,dim=-1)\n",
        "  torch.manual_seed(123)\n",
        "  sample = [torch.multinomial(probas,num_samples=1).item() for _ in range(1000)]\n",
        "  sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "  for i,freq in enumerate(sampled_ids):\n",
        "    print(freq,iv[i])\n",
        "\n",
        "def scale_temp(logits,temperature):\n",
        "  scaled_logits = logits/temperature\n",
        "  probas = torch.softmax(scaled_logits,dim=-1)\n",
        "  return probas\n",
        "\n",
        "temperatures = [1,0.1,5]\n",
        "sp = [scale_temp(next_token_logits, T) for T in temperatures]\n",
        "sptensor = torch.stack(sp)\n",
        "\n",
        "x = torch.arange(len(v))\n",
        "bw = 0.15\n",
        "fig,ax = plt.subplots(figsize=(5,3))\n",
        "for i,T in enumerate(temperatures):\n",
        "  rect = ax.bar(x+i*bw,sp[i],bw,label = f'{T}')\n",
        "\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(v.keys(), rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "fBea7vdbEa9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############write by own\n",
        "\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "C_ZI360j75wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = torch.topk(next_tokl,k=3)\n",
        "# n\n",
        "new_logits = torch.where(condition=next_token_logits<n[0][-1],input = torch.tensor(float('-inf')),other = next_token_logits)\n",
        "new_logits\n",
        "topk_probas = torch.softmax(new_logits,dim=-1)\n",
        "topk_probas"
      ],
      "metadata": {
        "id": "bETGL0DoNar7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 ############write by own\n",
        "n = torch.topk(next_tokl,3)\n",
        "cond = (next_tokl<n.values[-1])\n",
        "cond\n",
        "new = torch.where(cond,torch.tensor(float('-inf')),next_tokl)\n",
        "torch.softmax(new,dim=-1)\n",
        "#############write by own\n",
        "# test = torch.tensor([1,2,3,4])\n",
        "# cond = test>2\n",
        "# torch.where(cond,test,0)\n"
      ],
      "metadata": {
        "id": "q6CBcsTx76Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26 ############write by own\n",
        "def generate(model,device,start_text, max_new_tokens, tokenizer,context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "  model.eval()\n",
        "  idx = text2token(start_text,tokenizer).to(device)\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond =idx[:,-context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "    care = logits[:,-1,:]\n",
        "    if top_k>0:\n",
        "      n = torch.topk(care,top_k, dim=-1)\n",
        "      min_val = n.values[0][-1]\n",
        "      mask = care >= min_val\n",
        "      # return min_val.shape,care.shape,min_val\n",
        "      care = torch.where(mask,torch.tensor(float('-inf')).to(care.device),care)\n",
        "\n",
        "    if temperature>0:\n",
        "      care = care/temperature\n",
        "      probas = torch.softmax(care,dim=-1)\n",
        "      next_token_idx = torch.multinomial(probas,num_samples=1)\n",
        "\n",
        "    else:\n",
        "      probas = torch.softmax(care,dim=-1)\n",
        "      next_token_idx = torch.argmax(probas,dim=-1,keepdim=True)\n",
        "\n",
        "    idx = torch.cat((idx,next_token_idx),-1)\n",
        "  restext= token2text(idx,tokenizer)\n",
        "  return restext.replace(\"\\n\", \" \")\n",
        "\n",
        "\n",
        "\n",
        "#############write by own\n"
      ],
      "metadata": {
        "id": "6ThnIqLn763R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##load weight"
      ],
      "metadata": {
        "id": "JuGrI02LGiVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 27 ############write by own\n",
        "torch.save(model.state_dict(),'model.pth')\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "2FnaDUIX78bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28 ############write by own\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n",
        "model_load = GPTModel(GPT_CONFIG_124M)\n",
        "model_load.load_state_dict(torch.load('model.pth',map_location=device))\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "SZAPr-qf789k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29 ############write by own\n",
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "Telwbr5I7-VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 ############write by own\n",
        "print(settings)\n",
        "print(params.keys())\n",
        "print((params['wpe'].shape))\n",
        "print(len(params['blocks']))\n",
        "print((params['b'].shape))\n",
        "print((params['blocks'][2].keys()))\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "qtsXWyPhTHDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31 ############write by own\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},}\n",
        "\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})\n",
        "NEW_CONFIG.update({\"context_length\":1024})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "B5WLBcoT7-_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32 ############write by own\n",
        "def assign(left,right):\n",
        "  if left.shape != right.shape:\n",
        "    raise ValueError(f'Error, Left:{left.shape} Right:{right.shape}')\n",
        "  else:\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "#############write by own\n",
        "# a = torch.tensor([1.0,1.0,1.0])\n",
        "# b = torch.tensor([1.0,1.0,1.0])\n",
        "# pb = assign(a,b),\n",
        "# b,pb"
      ],
      "metadata": {
        "id": "aoMKIhPS7_in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG"
      ],
      "metadata": {
        "id": "6-PO5Pq9aFez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############write by own\n",
        "\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "ywb25RoG8BO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "pAP6eAWIZqjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############write by own\n",
        "\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "HCI3PR1b8BzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n",
        "\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "fsAMI2oQZyxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############write by own\n",
        "generate(model=gpt,start_text=\"my name is\",device=device,tokenizer=tk,max_new_tokens=15,context_size=GPT_CONFIG_124M[\"context_length\"],temperature=0.2,top_k=0)\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "5bW3RmUI8CcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############write by own\n",
        "\n",
        "#############write by own"
      ],
      "metadata": {
        "id": "V-ijxQhf8DZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2U0J1CaZxe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 5 All**"
      ],
      "metadata": {
        "id": "o5-ZknXW3x4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jP6nq42MGl5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch import nn as nn\n",
        "import tiktoken\n",
        "tk = tiktoken.get_encoding('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gd = {\n",
        "    'vocab_size':50257,\n",
        "    'context_length':256,\n",
        "    'emb_dim':768,\n",
        "    'n_heads':12,\n",
        "    'n_layers':12,\n",
        "    'drop_rate':0.1,\n",
        "    'qkv_bias':False\n",
        "}"
      ],
      "metadata": {
        "id": "3HGy6miINEk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Forward"
      ],
      "metadata": {
        "id": "AGo2zU760MIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MHA(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.wq = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.wk = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.wv = nn.Linear(d_in,d_out,qkv_bias)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones((context_length,context_length)),diagonal=1 ))\n",
        "    self.out_proj = nn.Linear(d_out,d_out)\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,seq_len,d_in = x.shape\n",
        "    mask_f = self.mask[:seq_len,:seq_len].bool()\n",
        "    q = self.wq(x)\n",
        "    k = self.wk(x)\n",
        "    v = self.wv(x)\n",
        "\n",
        "    q = q.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "    k = k.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "    v = v.view(b,seq_len,self.num_heads,self.head_dim)\n",
        "\n",
        "    q = q.transpose(1,2)\n",
        "    k = k.transpose(1,2)\n",
        "    v = v.transpose(1,2)\n",
        "\n",
        "    attn_score = q@k.transpose(2,3)\n",
        "    attn_score.masked_fill_(mask_f,-torch.inf)\n",
        "    attn_weights = torch.softmax(attn_score/k.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    output = attn_weights@v\n",
        "    output = output.transpose(1,2)\n",
        "    output = output.contiguous().view(b,seq_len,self.d_out)\n",
        "    output = self.out_proj(output)\n",
        "    return output\n",
        "\n",
        "class NormLayer(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
        "    self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1,keepdim=True,unbiased=False)\n",
        "    norm =(x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*norm +self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FF(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(emb_dim,4*emb_dim),\n",
        "        GELU(),\n",
        "      nn.Linear(4*emb_dim,emb_dim)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "\n",
        "class TFB(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.normlayer1 = NormLayer(cfg['emb_dim'])\n",
        "    self.normlayer2 = NormLayer(cfg['emb_dim'])\n",
        "    self.mha = MHA(d_in=cfg['emb_dim'],d_out=cfg['emb_dim'],context_length=cfg['context_length'],dropout=cfg['drop_rate'],num_heads=cfg['n_heads'],qkv_bias=cfg['qkv_bias'])\n",
        "    self.dropout = nn.Dropout(cfg['drop_rate'])\n",
        "    self.ff = FF(cfg['emb_dim'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    o_inp = x\n",
        "    x = self.normlayer1(x)\n",
        "    x = self.mha(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x+o_inp\n",
        "    o_inp = x\n",
        "    x = self.normlayer2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x+o_inp\n",
        "    return x\n",
        "\n",
        "\n",
        "class DGPT(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.token_emb_matrix = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "    self.pos_emb_matrix = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "    self.dropout = nn.Dropout(cfg['drop_rate'])\n",
        "    self.tfb = nn.Sequential(*[TFB(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "    self.finalNorm = NormLayer(cfg['emb_dim'])\n",
        "    self.outLayer = nn.Linear(cfg['emb_dim'],cfg['vocab_size'])\n",
        "  def forward(self,inp):\n",
        "    batch_size, seq_len = inp.shape\n",
        "    token_embedding = self.token_emb_matrix(inp)\n",
        "    pos_embedding = self.pos_emb_matrix(torch.arange(seq_len,device=inp.device))\n",
        "    x = token_embedding+pos_embedding\n",
        "    x = self.dropout(x)\n",
        "    x = self.tfb(x)\n",
        "    x = self.finalNorm(x)\n",
        "    x = self.outLayer(x)\n",
        "    return x\n",
        "\n",
        "# model = DGPT(gd)\n",
        "# a = model(batch)\n",
        "# a.shape\n"
      ],
      "metadata": {
        "id": "PmihFufNNOUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data"
      ],
      "metadata": {
        "id": "lMatZ73Zegjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('the-verdict.txt','r',encoding='utf-8') as f:\n",
        "  txt = f.read()\n",
        "\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "class GDataset(Dataset):\n",
        "  def __init__(self,txt,tokenizer,window_size,stride):\n",
        "    super().__init__()\n",
        "    self.e = tokenizer.encode(txt)\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    for i in range(0,len(self.e)-window_size,stride):\n",
        "      self.x.append(torch.tensor(self.e[i:i+window_size]))\n",
        "      self.y.append(torch.tensor(self.e[i+1:i+window_size+1]))\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "\n",
        "def Gdataloader(txt,batch_size,window_size,stride,shuffle=True,drop_last=True,num_workers=0):\n",
        "  dataset = GDataset(txt,tk,window_size,stride)\n",
        "  dataloader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n",
        "\n",
        "split_idx = int(0.9*len(txt))\n",
        "train_data = txt[:split_idx]\n",
        "valid_data = txt[split_idx:]\n",
        "\n",
        "train_loader = Gdataloader(train_data,batch_size=2,window_size=gd['context_length'],stride=gd['context_length'],drop_last=True,shuffle=True,num_workers=0)\n",
        "valid_loader = Gdataloader(valid_data,batch_size=2,window_size=gd['context_length'],stride=gd['context_length'],drop_last=False,shuffle=False,num_workers=0)\n",
        "\n",
        "# train_loader = Gdataloader(train_data,batch_size=2,window_size=10,stride=10,drop_last=True,shuffle=True,num_workers=0)\n",
        "# valid_loader = Gdataloader(valid_data,batch_size=2,window_size=10,stride=10,drop_last=False,shuffle=False,num_workers=0)\n",
        "\n",
        "\n",
        "for x,y in train_loader:\n",
        "  print(x.shape,y.shape)\n",
        "\n",
        "\n",
        "train_tokens=0\n",
        "for x,y in train_loader:\n",
        "  train_tokens+=x.numel()\n",
        "\n",
        "valid_tokens=0\n",
        "for x,y in valid_loader:\n",
        "  valid_tokens+=x.numel()\n",
        "train_tokens,valid_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LttznbTqeiJ0",
        "outputId": "8a048834-d66d-442e-a96d-86592e01a5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4608, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "tHarHq9djzSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Take a look"
      ],
      "metadata": {
        "id": "ZZIJ-CZ5p7Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def text2token(txt,tokenizer):\n",
        "  e = tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "  return torch.tensor(e).unsqueeze(0)\n",
        "\n",
        "def token2text(token,tokenizer):\n",
        "  token = token.squeeze(0)\n",
        "  token = token.tolist()\n",
        "  d = tokenizer.decode(token)\n",
        "  return d\n",
        "\n",
        "def generate_text(model,idx,context_length,max_new):\n",
        "  for _ in range(max_new):\n",
        "    curr_c = idx[:,-context_length:]\n",
        "    with torch.no_grad():\n",
        "      ce = model(curr_c)[:,-1,:]\n",
        "      ce = torch.softmax(ce,dim=-1)\n",
        "      next = torch.argmax(ce,dim=-1,keepdim=True)\n",
        "      idx = torch.cat((idx,next),dim=-1)\n",
        "  return idx\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "model = DGPT(gd)\n",
        "model = model.to(device)\n",
        "idx = text2token('Every effort moves',tokenizer=tk).to(device)\n",
        "gt = generate_text(model,idx,gd['context_length'],5)\n",
        "token2text(gt,tk)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A0Hp00cep9UF",
        "outputId": "dfae385d-73f0-43a9-edf4-c3443b0ebb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every effort moves inaction Wisdom Stores behaved gang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "-HgyJZCd0VGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##loss"
      ],
      "metadata": {
        "id": "JBzXIchNM4rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_loss(model,inp,target,device):\n",
        "  inp = inp.to(device)\n",
        "  target = target.to(device)\n",
        "  pred = model(inp)\n",
        "  loss = nn.functional.cross_entropy(pred.flatten(0,1),target.flatten(0,1))\n",
        "  return loss\n",
        "# for x,y in train_loader:\n",
        "#   print(cal_loss(model,x,y,device=device))\n",
        "\n",
        "def cal_loss_loader(model,dataloader,device,num_batch=None):\n",
        "  l = 0\n",
        "  if num_batch == 0:\n",
        "    return float(0.0)\n",
        "  elif num_batch == None:\n",
        "    num_batch = len(dataloader)\n",
        "  else:\n",
        "\n",
        "    num_batch = min(num_batch,len(dataloader))\n",
        "  for i,(x,y) in enumerate(dataloader):\n",
        "    if i>=num_batch:\n",
        "      break\n",
        "    else:\n",
        "      l+=cal_loss(model,x,y,device=device).item()\n",
        "  return l/num_batch\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "model = DGPT(gd)\n",
        "model = model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = cal_loss_loader(model,train_loader,device)\n",
        "  valid_loss = cal_loss_loader(model,valid_loader,device)\n",
        "print(valid_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6OXxXMX0Rjf",
        "outputId": "db9facb4-42f2-44df-daa1-26813769e142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.972859382629395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "LpzolRe0bX8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "model = DGPT(gd)\n",
        "model = model.to(device)\n",
        "\n",
        "def eval_model(model,train_loader,valid_loader,device,eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    tl = cal_loss_loader(model,train_loader,device,eval_iter)\n",
        "    vl = cal_loss_loader(model,valid_loader,device,eval_iter)\n",
        "  model.train()\n",
        "  return tl,vl\n",
        "\n",
        "def gen_sample(model,start_context,tokenizer,device):\n",
        "  model.eval()\n",
        "  idx = text2token(start_context,tokenizer).to(device)\n",
        "  text = token2text(generate_text(model,idx,1024,10),tokenizer)\n",
        "  print(text.replace('\\n',' '))\n",
        "  model.train()\n",
        "\n",
        "def train(model,tdata,vdata,num_epochs,optimizer,eval_freq,eval_iter,device):\n",
        "  train_loss,val_loss,track_tokens = [],[],[]\n",
        "  token_seens,global_step = 0,-1\n",
        "  model.train()\n",
        "  for e in range(num_epochs):\n",
        "    for x,y in tdata:\n",
        "      optimizer.zero_grad()\n",
        "      loss = cal_loss(model,x,y,device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      token_seens+=x.numel()\n",
        "      global_step+=1\n",
        "      if global_step % eval_freq == 0:\n",
        "        tl,vl = eval_model(model=model,train_loader=tdata,valid_loader=vdata,device=device,eval_iter=eval_iter)\n",
        "        train_loss.append(tl)\n",
        "        val_loss.append(vl)\n",
        "        track_tokens.append(token_seens)\n",
        "    print(f'Ep:{e+1} (Step {global_step}) Train loss:{tl} valid loss:{vl}')\n",
        "    # gen_sample(model,\"Every effort moves\",tk,device)\n",
        "  return train_loss,val_loss,track_tokens\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "tra_loss,vali_loss,track_tok = train(model=model,tdata=train_loader,vdata=valid_loader,num_epochs=10,optimizer=optimizer,eval_freq=len(train_loader),eval_iter=5,device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh9FGetjnpjr",
        "outputId": "f44e60dd-fd68-4765-e09b-3aa35f22de23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep:1 (Step 8) Train loss:9.80755443572998 valid loss:9.96696662902832\n",
            "Ep:2 (Step 17) Train loss:6.881672477722168 valid loss:7.21390962600708\n",
            "Ep:3 (Step 26) Train loss:5.777148628234864 valid loss:6.5198211669921875\n",
            "Ep:4 (Step 35) Train loss:4.684707927703857 valid loss:6.345846176147461\n",
            "Ep:5 (Step 44) Train loss:3.683500814437866 valid loss:6.1502227783203125\n",
            "Ep:6 (Step 53) Train loss:2.654703903198242 valid loss:6.187041282653809\n",
            "Ep:7 (Step 62) Train loss:1.7925708532333373 valid loss:6.213160514831543\n",
            "Ep:8 (Step 71) Train loss:1.1563263177871703 valid loss:6.313112258911133\n",
            "Ep:9 (Step 80) Train loss:0.659516429901123 valid loss:6.3861894607543945\n",
            "Ep:10 (Step 89) Train loss:0.39169758558273315 valid loss:6.5020856857299805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "# tra_loss,vali_loss,track_tok\n",
        "def plot(epochs,train_loss,valid_loss,track_tok):\n",
        "  fig,ax = plt.subplots(figsize = (8,5))\n",
        "\n",
        "  ax.plot(epochs,train_loss,label = 'tl')\n",
        "  ax.plot(epochs,valid_loss,linestyle = '--',label = 'vl')\n",
        "  ax.set_xlabel('Epochs')\n",
        "  ax.set_ylabel(\"Loss\")\n",
        "  ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "  ax.legend()\n",
        "\n",
        "  ax2 = ax.twiny()\n",
        "  ax2.plot(track_tok,train_loss)\n",
        "  ax2.set_xlabel('Token_seen')\n",
        "\n",
        "ep = torch.linspace(0,20,len(tra_loss))\n",
        "plot(ep,tra_loss,vali_loss,track_tok)\n",
        "\n",
        "# len(tra_loss)\n",
        "# len(tra_loss)\n",
        "# len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "lO5mqAKWUfHe",
        "outputId": "78588403-857d-4c93-ab0b-e83cac3b3441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optimize"
      ],
      "metadata": {
        "id": "Hyt2qL6leosv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "a = torch.randn(10).unsqueeze(0)\n",
        "a.shape\n",
        "n = torch.topk(a,5,dim=-1)\n",
        "# n.values[0][-1]\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knx8H41fF7_Y",
        "outputId": "6d13b768-a1ab-4a9b-87b8-be37215642ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,\n",
              "          0.3239, -0.1085]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a = torch.softmax(a,dim=-1)\n",
        "# torch.multinomial(a,5)\n",
        "\n",
        "mask = a<n.values[0][-1]\n",
        "a = a.masked_fill(mask,-torch.inf)\n",
        "a\n",
        "\n",
        "# n = torch.topk(care,top_k, dim=-1)\n",
        "# min_val = n.values[0][-1]\n",
        "# mask = care >= min_val\n",
        "# # return min_val.shape,care.shape,min_val\n",
        "# care = torch.where(mask,torch.tensor(float('-inf')).to(care.device),care)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4JYZrVnHFrG",
        "outputId": "49dc927e-fb30-4eea-e0c3-6fd751df00fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1115,  0.1204,    -inf,    -inf,    -inf,  0.2093,    -inf,    -inf,\n",
              "          0.3239, -0.1085]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.softmax(a,dim=-1)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBGmCYIpNGoB",
        "outputId": "268893f1-3b65-4e49-c03c-f8e1499471bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1616, 0.2038, 0.0000, 0.0000, 0.0000, 0.2227, 0.0000, 0.0000, 0.2498,\n",
              "         0.1621]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a[:,(torch.multinomial(a,1)).item()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KplbSAt5rRQj",
        "outputId": "784057c5-302a-4b1a-9e73-cc5c76f4db82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2038])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_all(model,idx,context_length,max_new,temprature = 0,topk = 0):\n",
        "  model.eval()\n",
        "  for _ in range(max_new):\n",
        "    curr_c = idx[:,-context_length:]\n",
        "    with torch.no_grad():\n",
        "      ce = model(curr_c)[:,-1,:]\n",
        "      if topk>0:\n",
        "        n = torch.topk(ce,topk,dim=-1).values\n",
        "        mask = ce<n[:,-1]\n",
        "        ce = ce.masked_fill(mask,-torch.inf)\n",
        "      if temprature>0:\n",
        "        ce = ce/temprature\n",
        "        ce = torch.softmax(ce,dim=-1)\n",
        "        next = torch.multinomial(ce,1)\n",
        "        idx = torch.cat((idx,next),dim=-1)\n",
        "      else:\n",
        "        ce = torch.softmax(ce,dim=-1)\n",
        "        next = torch.argmax(ce,dim=-1,keepdim=True)\n",
        "        idx = torch.cat((idx,next),dim=-1)\n",
        "  model.train()\n",
        "  return idx\n",
        "\n",
        "start = 'Every effort moves'\n",
        "id_start = text2token(start,tk).to(device)\n",
        "res =token2text(generate_all(model,id_start,gd['context_length'],10,temprature=0.5,topk=10),tk)\n",
        "res\n",
        "\n"
      ],
      "metadata": {
        "id": "6auF3_Xjeq3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8ce65a9-14cc-4df2-d490-43cd7f7616b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every effort moves the sunburnt cheeks furrowed by a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load model"
      ],
      "metadata": {
        "id": "udhH4xi_3Ng7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(),'model-my.pth')\n",
        "\n",
        "# device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "# model_my = DGPT(gd)\n",
        "# model_my.load_state_dict(torch.load('model-my.pth',map_location=device,weights_only=True))\n",
        "# model_my.to(device)\n",
        "\n",
        "# start = 'Every effort moves'\n",
        "# id_start = text2token(start,tk).to(device)\n",
        "# res =token2text(generate_all(model_my,id_start,gd['context_length'],10,temprature=0.5,topk=10),tk)\n",
        "# res\n"
      ],
      "metadata": {
        "id": "dstiQK3FtEkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd3LFMmQ-dJn",
        "outputId": "69ee0240-6a1e-45aa-9642-91f91129d333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 256,\n",
              " 'emb_dim': 768,\n",
              " 'n_heads': 12,\n",
              " 'n_layers': 12,\n",
              " 'drop_rate': 0.1,\n",
              " 'qkv_bias': False}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},}\n",
        "\n",
        "NEW_CONFIG = gd.copy()\n",
        "NEW_CONFIG.update(model_configs['gpt2-small (124M)'])\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})\n",
        "NEW_CONFIG.update({\"context_length\":1024})\n",
        "NEW_CONFIG\n",
        "LGPT = DGPT(NEW_CONFIG)"
      ],
      "metadata": {
        "id": "xTvSzHFs8QXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOGOhXuR-UBX",
        "outputId": "836fb54b-4ab6-4c94-fbfa-95ea89d010b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 172kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 981kiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 197kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:31<00:00, 5.43MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 10.6MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 569kiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 640kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3onyp194hofV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsCNgKMdhSH7",
        "outputId": "c42dfdcf-cedd-4860-8bac-4f75d5a93d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DGPT(\n",
              "  (token_emb_matrix): Embedding(50257, 768)\n",
              "  (pos_emb_matrix): Embedding(1024, 768)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (tfb): Sequential(\n",
              "    (0): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (finalNorm): NormLayer()\n",
              "  (outLayer): Linear(in_features=768, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(settings)\n",
        "print(params.keys())\n",
        "print((params['wpe'].shape))\n",
        "print(len(params['blocks']))\n",
        "print((params['b'].shape))\n",
        "print((params['blocks'][2].keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZA0NidMKpfl",
        "outputId": "ef5a3acb-4e2b-4de5-a428-0ce98befd1eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
            "(1024, 768)\n",
            "12\n",
            "(768,)\n",
            "dict_keys(['attn', 'ln_1', 'ln_2', 'mlp'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assign(left,right):\n",
        "  if left.shape != right.shape:\n",
        "    raise ValueError(f'left:{left.shape}, right:{right.shape}')\n",
        "  else:\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "def load_weights(gpt,params):\n",
        "  gpt.token_emb_matrix.weight = assign(gpt.token_emb_matrix.weight, params['wte'])\n",
        "  gpt.pos_emb_matrix.weight = assign(gpt.pos_emb_matrix.weight,params['wpe'])\n",
        "\n",
        "  for b in range(len(params['blocks'])):\n",
        "    q_w,k_w,v_w = np.split(params['blocks'][b]['attn']['c_attn']['w'],3,-1)\n",
        "    gpt.tfb[b].mha.wq.weight = assign(gpt.tfb[b].mha.wq.weight,q_w.T)\n",
        "    gpt.tfb[b].mha.wk.weight = assign(gpt.tfb[b].mha.wk.weight,k_w.T)\n",
        "    gpt.tfb[b].mha.wv.weight = assign(gpt.tfb[b].mha.wv.weight,v_w.T)\n",
        "    q_b,k_b,v_b = np.split(params['blocks'][b]['attn']['c_attn']['b'],3,-1)\n",
        "    gpt.tfb[b].mha.wq.bias = assign(gpt.tfb[b].mha.wq.bias,q_b)\n",
        "    gpt.tfb[b].mha.wk.bias = assign(gpt.tfb[b].mha.wk.bias,k_b)\n",
        "    gpt.tfb[b].mha.wv.bias = assign(gpt.tfb[b].mha.wv.bias,v_b)\n",
        "    gpt.tfb[b].mha.out_proj.weight =  assign(gpt.tfb[b].mha.out_proj.weight,params['blocks'][b]['attn']['c_proj']['w'].T)\n",
        "    gpt.tfb[b].mha.out_proj.bias =    assign(gpt.tfb[b].mha.out_proj.bias,params['blocks'][b]['attn']['c_proj']['b'])\n",
        "\n",
        "\n",
        "    gpt.tfb[b].ff.layers[0].weight =  assign(gpt.tfb[b].ff.layers[0].weight,params['blocks'][b]['mlp']['c_fc']['w'].T)\n",
        "    gpt.tfb[b].ff.layers[0].bias =    assign(gpt.tfb[b].ff.layers[0].bias,params['blocks'][b]['mlp']['c_fc']['b'])\n",
        "    gpt.tfb[b].ff.layers[2].weight =  assign(gpt.tfb[b].ff.layers[2].weight,params['blocks'][b]['mlp']['c_proj']['w'].T)\n",
        "    gpt.tfb[b].ff.layers[2].bias =    assign(gpt.tfb[b].ff.layers[2].bias,params['blocks'][b]['mlp']['c_proj']['b'])\n",
        "\n",
        "    gpt.tfb[b].normlayer1.scale =  assign(gpt.tfb[b].normlayer1.scale,params['blocks'][b]['ln_1']['g'])\n",
        "    gpt.tfb[b].normlayer1.shift =  assign(gpt.tfb[b].normlayer1.shift,params['blocks'][b]['ln_1']['b'])\n",
        "\n",
        "    gpt.tfb[b].normlayer2.scale =  assign(gpt.tfb[b].normlayer2.scale,params['blocks'][b]['ln_2']['g'])\n",
        "    gpt.tfb[b].normlayer2.shift =  assign(gpt.tfb[b].normlayer2.shift,params['blocks'][b]['ln_2']['b'])\n",
        "    gpt.finalNorm.scale = assign(gpt.finalNorm.scale, params['g'])\n",
        "    gpt.finalNorm.shift = assign(gpt.finalNorm.shift, params['b'])\n",
        "    gpt.outLayer.weight = assign(gpt.outLayer.weight,params['wte'])\n",
        "\n",
        "\n",
        "load_weights(LGPT,params)\n",
        "# params['wte'].shape,LGPT.outLayer.weight.shape\n",
        "# for i,j in LGPT.tfb[0].normlayer1.named_parameters():\n",
        "#   print(i,j.shape)\n",
        "# params['blocks'][0]['ln_1'].keys()\n",
        "# LGPT.tfb[0].ff.layers[2].weight.shape,params['blocks'][0]['mlp']['c_fc']['w'].shape\n",
        "# LGPT.outLayer"
      ],
      "metadata": {
        "id": "9PW0CDWw2Bu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i,j in LGPT.named_parameters():\n",
        "#   print(i,'  ',j.shape)\n",
        "\n",
        "LGPT.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyNIP7xqkkHD",
        "outputId": "a41507b0-84fe-4ba2-f6df-47e7afc849ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DGPT(\n",
              "  (token_emb_matrix): Embedding(50257, 768)\n",
              "  (pos_emb_matrix): Embedding(1024, 768)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (tfb): Sequential(\n",
              "    (0): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): TFB(\n",
              "      (normlayer1): NormLayer()\n",
              "      (normlayer2): NormLayer()\n",
              "      (mha): MHA(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (wq): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wk): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (wv): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (ff): FF(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (finalNorm): NormLayer()\n",
              "  (outLayer): Linear(in_features=768, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = 'What is my name'\n",
        "id_start = text2token(start,tk).to(device)\n",
        "res =token2text(generate_all(LGPT,id_start,gd['context_length'],10,temprature=0.5,topk=10),tk)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IXUTR9_fYWx1",
        "outputId": "68532de7-2f64-4fcb-fb30-f222bd41074c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is my name?\" she asked. \"I\\'m the one who'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 6**"
      ],
      "metadata": {
        "id": "KdsswWej61Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path= \"CFT.zip\"\n",
        "zfolder = 'CFT'\n",
        "with urllib.request.urlopen(url) as response:\n",
        "  with open(zip_path,'wb') as f:\n",
        "    f.write(response.read())\n",
        "\n",
        "with zipfile.ZipFile(zip_path,'r') as z:\n",
        "  z.extractall(zfolder)\n"
      ],
      "metadata": {
        "id": "X-s9iZYt9QqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "p = 'CFT/SMSSpamCollection'\n",
        "df = pd.read_csv(p,sep='\\t',header=None,names=['label','text'])\n",
        "def bd(df):\n",
        "  spam_num = df[df['label']=='spam'].shape[0]\n",
        "  ham_subset = df[df['label']=='ham'].sample(spam_num, random_state=123)\n",
        "  bd = pd.concat((ham_subset,df[df['label']=='spam']))\n",
        "  return bd\n",
        "bdata = bd(df)\n",
        "df['label'].value_counts()\n",
        "\n",
        "df[df['label']=='spam'].shape[0]\n",
        "bdata['label'].value_counts()\n",
        "bdata['label'] = bdata['label'].map({'ham':0,'spam':1})\n",
        "bdata['label'].value_counts()"
      ],
      "metadata": {
        "id": "c3KU5CBVAy55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mcQFIy8041t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bdata[bdata['label']==1])\n",
        "\n",
        "def random_split(df,train_frac,validation_frac):\n",
        "  df = df.sample(frac=1,random_state=123).reset_index(drop=True)\n",
        "  train_end = int(len(df)*train_frac)\n",
        "  validation_end = train_end+int(len(df)*validation_frac)\n",
        "  train_data = df[:train_end]\n",
        "  validation_data = df[train_end:validation_end]\n",
        "  test_data = df[validation_end:]\n",
        "  return train_data,validation_data,test_data\n",
        "\n",
        "a,b,c = random_split(bdata,0.7,0.1)\n",
        "a.to_csv('FT_train.csv',index=None)\n",
        "b.to_csv('FT_valid.csv',index=None)\n",
        "c.to_csv('FT_test.csv',index=None)\n",
        "len(c)"
      ],
      "metadata": {
        "id": "RkgKotcZNSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('FT_train.csv')\n",
        "import tiktoken\n",
        "tk = tiktoken.get_encoding('gpt2')\n",
        "a = [tk.encode(t) for t in test['text']]\n",
        "# len(a[0])\n",
        "print(len(a[5]))\n",
        "len(test)"
      ],
      "metadata": {
        "id": "wLw9c6rLnQQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "class FTdataset(Dataset):\n",
        "  def __init__(self,csv,tokenizer,max_length=None,pad_token = 50256):\n",
        "    self.df = pd.read_csv(csv)\n",
        "    self.encode_texts = [tokenizer.encode(text) for text in self.df['text']]\n",
        "    if max_length == None:\n",
        "      self.max_length = self._longest()\n",
        "    else:\n",
        "      self.max_length = max_length\n",
        "      self.encode_texts = [t[:self.max_length] for t in self.encode_texts]\n",
        "    self.encode_texts = [t+[pad_token]*(self.max_length-len(t)) for t in self.encode_texts]\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    t = torch.tensor(self.encode_texts[i])\n",
        "    l = torch.tensor(self.df['label'][i])\n",
        "    return t,l\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def _longest(self):\n",
        "    longest = 0\n",
        "    for i in self.encode_texts:\n",
        "      lo = len(i)\n",
        "      if longest<lo:\n",
        "        longest = lo\n",
        "    return longest\n",
        "\n",
        "train_d = FTdataset('FT_train.csv',tokenizer=tk)\n",
        "valid_d = FTdataset('FT_valid.csv',tokenizer=tk)\n",
        "test_d = FTdataset('FT_test.csv',tokenizer=tk)\n",
        "torch.manual_seed(123)\n",
        "# train_d.max_length\n",
        "train_l = DataLoader(train_d,batch_size=8,shuffle=True,num_workers=0,drop_last=True)\n",
        "valid_l = DataLoader(valid_d,batch_size=8,num_workers=0,drop_last=False)\n",
        "test_l = DataLoader(test_d , batch_size=8,num_workers=0,drop_last=False)\n",
        "len(train_l),len(valid_l),len(test_l)"
      ],
      "metadata": {
        "id": "du0K4ugdRL5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG"
      ],
      "metadata": {
        "id": "bdLFfzZLYsj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "NEW_CONFIG = gd.copy()\n",
        "NEW_CONFIG.update(model_configs['gpt2-small (124M)'])\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})\n",
        "NEW_CONFIG.update({\"context_length\":1024})\n",
        "NEW_CONFIG\n",
        "LGPT = DGPT(NEW_CONFIG)\n",
        "load_weights(LGPT,params)\n",
        "LGPT = LGPT.to(device)\n",
        "start = 'Every effort moves'\n",
        "id_start = text2token(start,tk).to(device)\n",
        "res =token2text(generate_all(LGPT,id_start,gd['context_length'],10,temprature=0.5,topk=10),tk)\n",
        "res"
      ],
      "metadata": {
        "id": "44kWyqLwhWkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LGPT"
      ],
      "metadata": {
        "id": "VD3AngAuZbcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in LGPT.parameters():\n",
        "  j.requires_grad = False\n",
        "torch.manual_seed(123)\n",
        "LGPT.outLayer = nn.Linear(NEW_CONFIG['emb_dim'],2)\n",
        "\n",
        "for j in LGPT.tfb[-1].parameters():\n",
        "  j.requires_grad = True\n",
        "for j in LGPT.finalNorm.parameters():\n",
        "  j.requires_grad = True\n",
        "\n"
      ],
      "metadata": {
        "id": "X7NdV7e5iJkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = 'Do you have time'\n",
        "enc = text2token(inp,tk).to(device)\n",
        "LGPT.to(device)\n",
        "LGPT.eval()\n",
        "with torch.no_grad():\n",
        "  out = LGPT(enc)\n",
        "logits = out[:,-1,:]\n",
        "# probas = torch.softmax(logits,dim=-1)\n",
        "label = torch.argmax(logits,dim=-1)\n",
        "label\n",
        "logits.shape\n",
        "label.shape"
      ],
      "metadata": {
        "id": "oljL8PxO0sWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_acc_loader(dataloader,model,device,num_batches):\n",
        "  model.to(device)\n",
        "\n",
        "  correct_prediction,num_example = 0,0\n",
        "  model.eval()\n",
        "  if num_batches==None:\n",
        "    num_batches=len(dataloader)\n",
        "  else:\n",
        "    num_batches = min(num_batches,len(dataloader))\n",
        "\n",
        "  for i,(j_inp,j_target) in enumerate(dataloader):\n",
        "    if i<num_batches:\n",
        "      j_inp = j_inp.to(device)\n",
        "      j_target = j_target.to(device)\n",
        "      with torch.no_grad():\n",
        "        logit = model(j_inp)[:,-1,:]\n",
        "        predict_label = torch.argmax(logit,dim=-1)\n",
        "        num_example+=predict_label.shape[0]\n",
        "        correct_prediction+=((predict_label == j_target).sum().item())\n",
        "    else:\n",
        "      break\n",
        "  return correct_prediction/num_example\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "LGPT.to(device)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_acc = calc_acc_loader(train_l,LGPT,device=device,num_batches=10)\n",
        "valid_acc = calc_acc_loader(valid_l,LGPT,device,num_batches=10)\n",
        "test_acc = calc_acc_loader(test_l,LGPT,device,num_batches=10)\n",
        "\n",
        "train_acc,valid_acc,test_acc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NmrbnldH1Xlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_ftloss(inp,tar,model,device):\n",
        "  inp = inp.to(device)\n",
        "  tar = tar.to(device)\n",
        "  logits = model(inp)[:,-1,:]\n",
        "  loss = torch.nn.functional.cross_entropy(logits,tar)\n",
        "  return loss\n",
        "\n",
        "def cal_ftloss_loader(dataloader,model,device,num_batches):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  if num_batches==None:\n",
        "    num_batches=len(dataloader)\n",
        "  else:\n",
        "    num_batches = min(num_batches,len(dataloader))\n",
        "  for i,(j_inp,j_target) in enumerate(dataloader):\n",
        "    if i<num_batches:\n",
        "      j_inp = j_inp.to(device)\n",
        "      j_target = j_target.to(device)\n",
        "      # logits = model(j_inp)[:,-1,:]\n",
        "      loss = cal_ftloss(j_inp,j_target,model,device)\n",
        "      total_loss+=loss\n",
        "    else:\n",
        "      break\n",
        "  return total_loss/num_batches\n",
        "with torch.no_grad():\n",
        "  train_loss = cal_ftloss_loader(train_l,LGPT,device=device,num_batches=5)\n",
        "  valid_loss = cal_ftloss_loader(valid_l,LGPT,device,num_batches=5)\n",
        "  test_loss  = cal_ftloss_loader(test_l,LGPT,device,num_batches=5)\n",
        "\n",
        "train_loss,valid_loss,test_loss"
      ],
      "metadata": {
        "id": "RysTTZ4W1Xn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ft(model,train_loader,valid_loader,device,eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = cal_ftloss_loader(train_loader,model,device,eval_iter)\n",
        "    valid_loss = cal_ftloss_loader(valid_loader,model,device,eval_iter)\n",
        "  model.train()\n",
        "  return train_loss,valid_loss\n",
        "\n",
        "def train_ft(model,train_loader,valid_loader,optimizer,device,num_epochs,eval_freq,eval_iter):\n",
        "  train_loss,valid_loss,train_accs,valid_accs = [],[],[],[]\n",
        "  example_seen,global_step = 0,0\n",
        "  for e in range(num_epochs):\n",
        "    model.train()\n",
        "    for inp,tar in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = cal_ftloss(inp,tar,model,device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      example_seen+=inp.shape[0]\n",
        "      global_step+=1\n",
        "\n",
        "      if global_step % eval_freq ==0:\n",
        "        tr_loss,val_loss = evaluate_ft(model,train_loader,valid_loader,device,eval_iter)\n",
        "        train_loss.append(tr_loss)\n",
        "        valid_loss.append(val_loss)\n",
        "        print(f\"Ep {e+1} (Step {global_step:06d}):\" f\"Train loss {tr_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "        train_acc = calc_acc_loader(train_loader,model,device,eval_iter)\n",
        "        valid_acc = calc_acc_loader(valid_loader,model,device,eval_iter)\n",
        "        train_accs.append(train_acc)\n",
        "        valid_accs.append(valid_acc)\n",
        "  return train_loss,valid_loss,train_accs,valid_accs,example_seen\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OhTIy0_pFPvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(LGPT.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "tl,vl,ta,va,exseen = train_ft(LGPT, train_l, valid_l, optimizer, device,num_epochs=num_epochs, eval_freq=50, eval_iter=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "S2j54_jEFPxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLKOP31kaMK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a second x-axis for examples seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(tl),device=\"cpu\")\n",
        "examples_seen_tensor = torch.linspace(0, exseen, len(tl),device=\"cpu\")\n",
        "tl = [i.to('cpu') for i in tl]\n",
        "vl = [i.to('cpu') for i in vl]\n",
        "# ta = [i.to('cpu') for i in ta]\n",
        "# va = [i.to('cpu') for i in va]\n",
        "\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, tl, vl)\n",
        "\n",
        "# tl,vl,ta,va,exseen"
      ],
      "metadata": {
        "id": "AwGrN6YkFPz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_values(epochs_tensor, examples_seen_tensor, ta, va)"
      ],
      "metadata": {
        "id": "BMJ7syvGFP2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.pos_emb_matrix.weight.shape[0]\n",
        "\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ],
      "metadata": {
        "id": "OpQC275DcJaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(classify_review(text_1, LGPT, tk, device, max_length=train_d.max_length))"
      ],
      "metadata": {
        "id": "SGBrYuKrclGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, LGPT, tk, device, max_length=train_d.max_length\n",
        "))"
      ],
      "metadata": {
        "id": "7E739ma-c7kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 7**"
      ],
      "metadata": {
        "id": "6Wemy-OVel5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "4dk_ZpUDeoVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        "fp = 'ift_data.json'\n",
        "with urllib.request.urlopen(url) as response:\n",
        "  text_data = response.read().decode('utf-8')\n",
        "\n",
        "with open(fp,'w',encoding='utf-8') as file:\n",
        "  file.write(text_data)\n",
        "\n",
        "with open(fp,'r') as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "data[:5],type(data),type(data[0]),len(data)\n",
        "for i in data[:5]:\n",
        "  for j in i:\n",
        "    print(j,i[j])\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "efYeOAeqeoXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8214abe2-c399-416d-865e-dd32b89a6e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "instruction Evaluate the following phrase by transforming it into the spelling given.\n",
            "input freind --> friend\n",
            "output The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n",
            "\n",
            "instruction Edit the following sentence for grammar.\n",
            "input He go to the park every day.\n",
            "output He goes to the park every day.\n",
            "\n",
            "instruction Convert 45 kilometers to meters.\n",
            "input \n",
            "output 45 kilometers is 45000 meters.\n",
            "\n",
            "instruction Rewrite this sentence to start with 'Although': Despite the rain, they went for a walk.\n",
            "input \n",
            "output Although it was raining, they went for a walk.\n",
            "\n",
            "instruction What are the first 10 square numbers?\n",
            "input \n",
            "output 1, 4, 9, 16, 25, 36, 49, 64, 81, 100.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def format_input(entry):\n",
        "#     instruction_text =  f\"Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "\n",
        "#     input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "#     return instruction_text + input_text\n",
        "\n",
        "# inp = format_input(data[999])\n",
        "# resp= f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
        "\n",
        "# print(inp+resp)\n"
      ],
      "metadata": {
        "id": "6RBdLW9yeoaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text\n",
        "\n",
        "inp = format_input(data[999])\n",
        "resp= f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
        "\n",
        "print(inp+resp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuqDwKKH3Vyz",
        "outputId": "4d796430-dfd8-440f-c475-7c799010963c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "### Response:\n",
            "An antonym of 'complicated' is 'simple'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)\n",
        "train_portion = int(len(data)*0.85)\n",
        "test_portion = int(len(data)*0.1)\n",
        "valid_portion = len(data)-train_portion-test_portion\n",
        "train_portion,test_portion,valid_portion\n",
        "\n",
        "train_ift_data = data[:train_portion]\n",
        "test_ift_data =  data[train_portion:train_portion+test_portion]\n",
        "valid_ift_data = data[train_portion+test_portion:]\n",
        "\n",
        "len(train_ift_data),len(test_ift_data),len(valid_ift_data)"
      ],
      "metadata": {
        "id": "F5Rt7J2neoco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30061036-b4de-48ba-9eb6-f9975ca55216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(935, 110, 55)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self,data,tokenizer):\n",
        "    super().__init__()\n",
        "    self.data = data\n",
        "    self.encoded_texts = []\n",
        "    for entry in data:\n",
        "      ins_plus_inp = format_input(entry)\n",
        "      resp_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = ins_plus_inp + resp_text\n",
        "      self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "  def __getitem__(self,indx):\n",
        "    return self.encoded_texts[indx]\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n"
      ],
      "metadata": {
        "id": "LY6CBDLJeofF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "# print(custom_collate_draft_1(batch))"
      ],
      "metadata": {
        "id": "Q8w7ZSnjeokE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "134NwjB_eomj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a,b"
      ],
      "metadata": {
        "id": "IRFzCK1lk0eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch,pad_id=50256,ignore_id=-100,allowed_max_length=None,device=device):\n",
        "  batch_max_length = max(len(item)+1 for item in batch)\n",
        "  inp_lst,tar_lst=[],[]\n",
        "  for item in batch:\n",
        "    new_item=item.copy()\n",
        "    new_item+=[pad_id]\n",
        "    padded = new_item+[pad_id]*(batch_max_length-len(new_item))\n",
        "    inp = torch.tensor(padded[:-1])\n",
        "    tar = torch.tensor(padded[1:])\n",
        "    mask = tar==pad_id\n",
        "    # mask[0]=True\n",
        "    # tar = torch.where(mask, ignore_id, tar)\n",
        "    indices=torch.nonzero(mask).squeeze()\n",
        "    if indices.numel()>1:\n",
        "      tar[indices[1:]]=ignore_id\n",
        "\n",
        "    if allowed_max_length is not None:\n",
        "      inp = inp[:allowed_max_length]\n",
        "      tar = inp[:allowed_max_length]\n",
        "    inp_lst.append(inp)\n",
        "    tar_lst.append(tar)\n",
        "\n",
        "  inp_lst = torch.stack(inp_lst).to(device)\n",
        "  tar_lst = torch.stack(tar_lst).to(device)\n",
        "  return inp_lst,tar_lst\n",
        "\n",
        "\n",
        "\n",
        "custom_collate_fn(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otCLgW9hlDcT",
        "outputId": "2668f097-5224-4f95-abf3-47e8a98f56ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    0,     1,     2,     3,     4],\n",
              "         [    5,     6, 50256, 50256, 50256],\n",
              "         [    7,     8,     9, 50256, 50256]], device='cuda:0'),\n",
              " tensor([[    1,     2,     3,     4, 50256],\n",
              "         [    6, 50256,  -100,  -100,  -100],\n",
              "         [    8,     9, 50256,  -100,  -100]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ift_data),len(valid_ift_data),len(test_ift_data)\n",
        "\n",
        "num_workers=0\n",
        "batch_size=8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_ift_set = InstructionDataset(train_ift_data,tokenizer=tk)\n",
        "valid_ift_set = InstructionDataset(valid_ift_data,tokenizer=tk)\n",
        "test_ift_set = InstructionDataset(test_ift_data,tokenizer=tk)\n",
        "\n",
        "train_ift_loader = DataLoader(train_ift_set,batch_size=batch_size,collate_fn=custom_collate_fn,shuffle=True,num_workers=num_workers,drop_last=True)\n",
        "valid_ift_loader = DataLoader(valid_ift_set,batch_size=batch_size,collate_fn=custom_collate_fn,shuffle=False,num_workers=num_workers,drop_last=False)\n",
        "test_ift_loader = DataLoader(test_ift_set,batch_size=batch_size,collate_fn=custom_collate_fn,shuffle=False,num_workers=num_workers,drop_last=False)\n"
      ],
      "metadata": {
        "id": "ywxo4DkFp1Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i,t in train_ift_loader:\n",
        "#   print(i.device,t.device)"
      ],
      "metadata": {
        "id": "yJ4FKmULqIp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHZn1tlj4iD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(),'model-my.pth')\n",
        "\n",
        "# device = 'cuda' if (torch.cuda.is_available()) else 'cpu'\n",
        "# model_my = DGPT(gd)\n",
        "# model_my.load_state_dict(torch.load('model-my.pth',map_location=device,weights_only=True))\n",
        "# model_my.to(device)\n",
        "\n",
        "# start = 'Every effort moves'\n",
        "# id_start = text2token(start,tk).to(device)\n",
        "# res =token2text(generate_all(model_my,id_start,gd['context_length'],10,temprature=0.5,topk=10),tk)\n",
        "# res\n"
      ],
      "metadata": {
        "id": "qEb-z5xA430V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "bzkCjUi_Re-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(gd)\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},}\n",
        "\n",
        "NEW_CONFIG = gd.copy()\n",
        "NEW_CONFIG.update(model_configs['gpt2-small (124M)'])\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})\n",
        "NEW_CONFIG.update({\"context_length\":1024})\n",
        "# print(NEW_CONFIG)\n",
        "# print(settings)\n",
        "# print(params.keys())\n",
        "# print((params['wpe'].shape))\n",
        "# print(len(params['blocks']))\n",
        "# print((params['b'].shape))\n",
        "# print((params['blocks'][2].keys()))\n",
        "IFTPT = DGPT(NEW_CONFIG)\n",
        "load_weights(IFTPT,params)\n",
        "IFTPT.to(device)\n",
        "\n",
        "start = 'Today is'\n",
        "id_start = text2token(start,tk).to(device)\n",
        "res =token2text(generate(model=IFTPT, idx = id_start,context_size=gd['context_length'],max_new=35,temperature=0.5,top_k=10),tk)\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2b4897a-28f4-4c5e-c6ad-a6154a75de89",
        "id": "uGHksGhz430V"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Today is a good time to stop by the store, as the, you know, a good time to buy a pair of shoes.\\n\\nYou know, you know, the,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrJwdSQmRfBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NQWqBvpRfFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9K-p0XlHRfIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-AWVjA0dRfKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = format_input(valid_ift_data[0])\n",
        "print(input_text)\n",
        "# torch.manual_seed(123)\n",
        "start = input_text\n",
        "id_start = text2token(start,tk).to(device)\n",
        "res =token2text(generate_all(IFTPT,id_start,gd['context_length'],max_new=35,temprature=0.5,topk=10),tk).strip()\n",
        "print(\"*****************************\")\n",
        "print(res)\n"
      ],
      "metadata": {
        "id": "yYNrjd226Nie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8e067e-01a1-47b1-c7b6-ccdb30969753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "*****************************\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Response:\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "### Response:\n",
            "\n",
            "Write a response that appropriately completes the request.\n",
            "\n",
            "###\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train_IFT"
      ],
      "metadata": {
        "id": "2d7z8gRB8zDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cal_loss_loader(model,dataloader,device,num_batch=None)\n",
        "torch.manual_seed(123)\n",
        "IFTPT.to(device)\n",
        "with torch.no_grad():\n",
        "  tloss = cal_loss_loader(IFTPT,train_ift_loader,device=device,num_batch=5)\n",
        "  vloss = cal_loss_loader(IFTPT,valid_ift_loader,device=device,num_batch=5)\n",
        "print(tloss,vloss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywRcBxWL76k_",
        "outputId": "2e2d4608-dddb-4e1f-943e-30d8d0ba70fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.2354127883911135 4.0743828296661375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zag7yk_XFqYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(IFTPT.parameters(),lr = 0.00005,weight_decay=0.01)\n",
        "num_epochs=5"
      ],
      "metadata": {
        "id": "Y7eerIui9JxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b,c = train(IFTPT,train_ift_loader,valid_ift_loader,num_epochs,optimizer,eval_freq=5,eval_iter=None,device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9EHljPy76rs",
        "outputId": "54df9101-e1bc-43cd-fae4-925fc1764410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep:1 (Step 115) Train loss:0.6952929995183287 valid loss:0.8029652323041644\n",
            "Ep:2 (Step 231) Train loss:0.5370898896764065 valid loss:0.7446662613323757\n",
            "Ep:3 (Step 347) Train loss:0.4375175689828807 valid loss:0.7166927967752729\n",
            "Ep:4 (Step 463) Train loss:0.3595290767221615 valid loss:0.71275680405753\n",
            "Ep:5 (Step 579) Train loss:0.30496822246189775 valid loss:0.71815082856587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "for entry in test_ift_data[:3]:\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "\n",
        "    token_ids = generate(IFTPT,idx=text2token(input_text, tk).to(device),max_new=256,context_size=gd[\"context_length\"],eos_id=50256)\n",
        "    generated_text = token2text(token_ids, tk)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Output:\", \"\").strip()\n",
        "    # response_text = generated_text[len(input_text):]\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coxkdbIQ76t-",
        "outputId": "c2f07c30-ec73-4ae6-b03f-9f443d9b3ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is very fast.\n",
            "\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "###\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "\n",
            "###\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> Clouds are the most common type of cloud. They are usually associated with a cloud that is located in the cloud.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the cloud.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud cloud. They are located in the cloud.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the cloud.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the cloud.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the cloud.\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the cloud.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the cloud.\n",
            "\n",
            "### Cloud type:\n",
            "\n",
            "Clouds are the most common type of cloud. They are located in the cloud.\n",
            "\n",
            "### Cloud type\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The name of the task is to be given to the user.\n",
            "\n",
            "### The author of 'Pride and Prejudice'.\n",
            "The author of the task is to be given the name of the author of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the author of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author of the task is to be given the name of the task.\n",
            "\n",
            "### The author\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5ZY3rFYMrFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "6QWHYTio7635"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIsLRA3Y7669"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgGXeyFi769m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "hxVSqaEuMyuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A"
      ],
      "metadata": {
        "id": "Nn8cmeXzgazB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a = torch.tensor([[1,2,3],[4,5,6]])\n",
        "a.shape\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOj2kWlgcuX",
        "outputId": "64261f84-ced5-40a5-97f3-c2d8d5dd211d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a.view(3,2)\n",
        "# a.reshape(3,2)\n",
        "# a = torch.tensor([1,2,3,4,5,6])\n",
        "# a.shape\n",
        "# a.reshape(3,2)\n",
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([4,5,6])\n",
        "c = torch.tensor([7,8,9])\n",
        "\n",
        "a.unsqueeze_(0)\n",
        "b.unsqueeze_(0)\n",
        "c.unsqueeze_(0)\n",
        "d1 = torch.cat((a,b,c),dim=0)\n",
        "d2 = d1\n",
        "\n",
        "d = torch.stack((d1,d2),dim=0)\n",
        "d\n",
        "# a.shape\n",
        "# torch.stack((a,b),dim=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em4-rUs8gpkv",
        "outputId": "550f7a42-3986-47ea-9da5-b5262c8272d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3],\n",
              "         [4, 5, 6],\n",
              "         [7, 8, 9]],\n",
              "\n",
              "        [[1, 2, 3],\n",
              "         [4, 5, 6],\n",
              "         [7, 8, 9]]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "# from torch.autograd import grad\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2],requires_grad=True)\n",
        "b = torch.tensor([0.0],requires_grad=True)\n",
        "\n",
        "# w1 = torch.tensor([2.2])\n",
        "# b = torch.tensor([0.0])\n",
        "\n",
        "\n",
        "z = torch.sigmoid(x1*w1+b)\n",
        "loss = F.binary_cross_entropy(z,y)\n",
        "loss.backward()\n",
        "w1.grad,b.grad\n",
        "\n",
        "# dw1 = grad(loss,w1,retain_graph=True)\n",
        "# db = grad(loss,b,retain_graph=True)\n",
        "# db\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNwUUn2ngp_m",
        "outputId": "16be67ed-6a26-4e7c-fd07-7c3591184ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.0898]), tensor([-0.0817]))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "NoKTjc22gqD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self,inc,out):\n",
        "    super().__init__()\n",
        "    self.NN = nn.Sequential(\n",
        "        nn.Linear(inc,30,bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(30,20,bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(20,out)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.NN(x)\n",
        "\n",
        "m = NN(50,3)\n",
        "\n",
        "for i,j in m.named_parameters():\n",
        "  print(i)\n",
        "\n",
        "sum(p.numel() for p in m.parameters())\n",
        "print(m.NN[0].weight.shape)\n",
        "# a = torch.tensor([[1,2,3,4],[5,6,7,8],[0,0,0,0]])\n",
        "# [p.numel() for p in a]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsshMReYgqG1",
        "outputId": "8dd4661b-8d9f-42d8-843e-a0e5859639a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN.0.weight\n",
            "NN.0.bias\n",
            "NN.2.weight\n",
            "NN.2.bias\n",
            "NN.4.weight\n",
            "NN.4.bias\n",
            "torch.Size([30, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.randn((1,50))\n",
        "with torch.no_grad():\n",
        "  out = m(x)\n",
        "out = torch.softmax(out,dim=-1)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P6ld9NofbjI",
        "outputId": "6a6c04f8-75bd-41ba-918c-e7d8e29e05f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3753, 0.3453, 0.2794]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor([[-1.2,3.1],[-0.9,2.9],[-0.5,2.6],[2.3,-1.1],[2.7,-1.5]])\n",
        "\n",
        "y_train=torch.tensor([0,0,0,1,1])\n",
        "\n",
        "x_test = torch.tensor([[-0.8,2.8],[2.6,-1.6]])\n",
        "y_test = torch.tensor([0,1])\n",
        "\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class Toy(Dataset):\n",
        "  def __init__(self,X,Y):\n",
        "    super().__init__()\n",
        "    self.x = X\n",
        "    self.y = Y\n",
        "  def __getitem__(self,indx):\n",
        "    return self.x[indx],self.y[indx]\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "train_toy = Toy(x_train,y_train)\n",
        "test_toy = Toy(x_test,y_test)\n",
        "\n",
        "len(train_toy)\n",
        "\n",
        "tr_loader = DataLoader(train_toy,batch_size=2,shuffle=True,num_workers=0,drop_last=True)\n",
        "test_loader = DataLoader(test_toy,batch_size=2,shuffle=False,num_workers=0,)\n",
        "\n",
        "# for i,j in test_loader:\n",
        "#   print(i,j)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "tnn = NN(2,2)\n",
        "optimizer = torch.optim.SGD(tnn.parameters(),lr=0.5)\n",
        "num_epochs=10\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  tnn.train()\n",
        "  for i,(x,y) in enumerate(tr_loader):\n",
        "    logit = tnn(x)\n",
        "    loss = F.cross_entropy(logit,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"ep:{e+1} batch:{i}/ {len(tr_loader)} loss: {loss}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t32vhhZlga7k",
        "outputId": "dd8c3b6f-5fd4-45be-a008-7b1a39d3b2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep:1 batch:0/ 2 loss: 0.7487356662750244\n",
            "ep:1 batch:1/ 2 loss: 0.6450306177139282\n",
            "ep:2 batch:0/ 2 loss: 0.44225579500198364\n",
            "ep:2 batch:1/ 2 loss: 0.12562280893325806\n",
            "ep:3 batch:0/ 2 loss: 0.026905544102191925\n",
            "ep:3 batch:1/ 2 loss: 0.0043272399343550205\n",
            "ep:4 batch:0/ 2 loss: 0.01814688928425312\n",
            "ep:4 batch:1/ 2 loss: 0.01870044507086277\n",
            "ep:5 batch:0/ 2 loss: 0.0011886745924130082\n",
            "ep:5 batch:1/ 2 loss: 0.019673030823469162\n",
            "ep:6 batch:0/ 2 loss: 0.011796331033110619\n",
            "ep:6 batch:1/ 2 loss: 0.0025485435035079718\n",
            "ep:7 batch:0/ 2 loss: 0.0009615547023713589\n",
            "ep:7 batch:1/ 2 loss: 0.008330076932907104\n",
            "ep:8 batch:0/ 2 loss: 0.0037992321886122227\n",
            "ep:8 batch:1/ 2 loss: 0.0008084033615887165\n",
            "ep:9 batch:0/ 2 loss: 0.0023475568741559982\n",
            "ep:9 batch:1/ 2 loss: 0.0017554151127114892\n",
            "ep:10 batch:0/ 2 loss: 0.005513850133866072\n",
            "ep:10 batch:1/ 2 loss: 0.001594828674569726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2AceJ86cga-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WKlpha4ogbBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WW4_ojngbEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ulW-WtQgbG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__AWaOy2gbJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JHZ1vzwagbMN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}